[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Naive Set Theory",
    "section": "",
    "text": "Preface by the Editor\nAs the book title says, this is the famous set theory book Naive Set Theory by Paul Richard Halmos, first published in 1960 by D. Van Nostrand Company, INC., part of a series called The University Series in Undergraduate Mathematics.\nWhat the title doesn’t say is that this version is an independent re-edition. The original work is currently public domain in Hathi Trust Digital Library — the reader probably found (or could find) the original digitized book on Google by just searching for its title. This version was written in LaTeX and first released on July 14, 2023, available for free to download on my Github repository. After the initial release, some people have already made contributions in fixing typos and further improving the re-edition. I extend here my gratitude to these people for helping me and keeping the spirit of the re-edition alive.\nEven though the book was freely available online, there are three reasons for this project. First, the book in its digitized state is perfectly readable, but it doesn’t allow searching words with  and doesn’t have an interactable summary with it. The second is to update the book by correcting the errors in the original version, following the published errata - as noticed, and updated in this edition, by Michał Zdunek. The third reason is purely personal, I have a passion and gratitude for this book and, while I want to learn OCR, I decided to re-edit it as a homage.\nSome notes on this re-edition are necessary. The book page format is B5 paper with font size 12pt. The margins of the book should be perfectly suitable for printing. The mainly differences with the original editions are the cover and the chapters title page designs. The mathematical symbol which denotes in the original is (\\(\\epsilon\\)), but I opted to use (\\(\\in\\)) since it’s used regularly nowadays for this case. Besides this, I didn’t change anything from the text. Therefore, any mistakes — which I hope are non-existent or, at least, few — are solely mine, and if someone finds any please contact me via e-mail.\nAs mentioned, the original book is public domain and, so, freely available in the internet. Therefore, the resulting re-edition of the book at the end of this project has no lucrative ends by any means. This re-edition cannot be used for any commercial purposes.\nI thought about writting a short story about Paul R. Halmos, since it’s common for books to do this specially after the author has deceased. However, I couldn’t do a better job than someone just searching on Google and/or Wikipedia. So, for now, I will just say that this book has a special place in my heart. It was one of the first works that introduced and helped me push througth writting proofs. And at the end, I fell in love not only with it, but with mathematics overall. I hope that anybody that found this version can have the same outcome as I did. Now read it, absorb it and forget it.\n\nMatheus Girola Macedo Barbosa - 01/07/2024",
    "crumbs": [
      "Preface by the Editor"
    ]
  },
  {
    "objectID": "preface.html",
    "href": "preface.html",
    "title": "Preface by the Author",
    "section": "",
    "text": "Every mathematician agrees that every mathematician must know some set theory; the disagreement begins in trying to decide how much is some. This book contains my answer to that question. The purpose of the book is to tell the beginning student of advanced mathematics the basic set-theoretic facts of life, and to do so with the minimum of philosophical discourse and logical formalism. The point of view throughout is that prospective mathematician anxious to study groups, or integrals, or manifolds. From this point of view the concepts and methods of this book are merely some of the standard mathematical tools; the expert specialist will find nothing new here.\nScholarly bibliographical credits and references are out of place in a purely expository book such as this one. The student who gets interested in set theory for its own sake should know, bowever, that there is much more to the subject than there is in this book. One of the most beautiful sources of set-theoretic wisdom is still Hausdorff’s Set theory. A recent and highly readable addition to the literature, with an extensive and up-to-date bibliography, is Axiomatic set theory by Suppes.\nIn set theory “naive” and “axiomatic” are contrasting words. The present treatment might best be described as axiomatic set theory from the naive point of view. It is axiomatic in that some axioms for set theory are stated and used as the basis of all subsequent proofs. It is naive in that the language and notation are those of ordinary informal (but formalizable) mathematics. A more important way in which the naive point view predominates is that set theory is regarded as a body of facts, of which the axioms are a brief and convenient summary; in the orthodox axiomatic view the logical relations among various axioms are the central objects of study. Analogously, a study of geometry might be regarded purely naive if it proceeded on the paper-folding kind of intuition alone; the other extreme, the purely axiomatic one, is the one in which axioms for the various non-Euclidean geometries are studied with the same amount of attention as Euclid’s. The analogue of the point of view of this book is the study of just one sane set of axioms with the intention of describing Euclidean geometry only.\nInstead of Naive set theory a more honest title for the book would have been An outline of the elements of naive set theory. “Elements” would warn the reader that not everything is here; “outline” would warn him that even what is here needs filling in. The style is usually informal to the point of conversational. There are very few displayed theorems; most of the facts are just stated and followed by a sketch of a proof, very much as they might be in a general descriptive lecture. There are only a few exercises, officially so labelled, but, in fact, most of the book is nothing but a long chain of exercises with hints. The reader should continually ask himself whether he knows how to jump from one hint to the next, and, accordingly, he should not be discouraged if he finds that his reading rate is considerably slower than normal.\nThis is not to say that the contents of this book are unusually difficult or profound. What is true is that the concepts are very general and very abstract, and that, therefore, they may take some getting used to. It is a mathematical truism, however, that the more generally a theorem applies, the less deep it is. The student’s task in learning set theory is to steep himself in unfamiliar but essentially shallow generalities till they become so familiar that they can be used with almost no conscious effort. In other words, general set theory is pretty trivial stuff really, but, if you want to be a mathematician, you need some, and here it is; read it, absorb it, and forget it.\n\nP. R. H.",
    "crumbs": [
      "Preface by the Author"
    ]
  },
  {
    "objectID": "chapter_1.html",
    "href": "chapter_1.html",
    "title": "1  The Axiom of Extension",
    "section": "",
    "text": "A pack of wolves, a bunch of grapes, or a flock of pigeons are all examples of sets of things. The mathematical concept of a set can be used as the foundation for all known mathematics. The purpose of this little book is to develop the basic properties of sets. Incidentally, to avoid terminological monotony, we shall sometimes say collection instead of set. The word “class” is also used in this context, but there is a slight danger in doing so. The reason is that in some approaches to set theory “class” has a special technical meaning. We shall have occasion to refer to this again a little later.\nOne thing that the development will not include is a definition of sets. The situation is analogous to the familiar axiomatic approach to elementary geometry. That approach does not offer a definition of points and lines; instead it describes what it is that one can do with those objects. The semi-axiomatic point of view adopted here assumes that the reader has the ordinary, human, intuitive (and frequently erroneous) understanding of what sets are; the purpose of the exposition is to delineate some of the many things that one can correctly do with them.\nSets, as they are usually conceived, have elements or members. An element of a set may be a wolf, a grape, or a pigeon. It is important to know that a set itself may also be an element of some other set. Mathematics is full of examples of sets of sets. A line, for instance; is a set of points; the set of all lines in the plane is a natural example of a set of sets (of points). What may be surprising is not so much that sets may occur as elements, but that for mathematical purposes no other elements need ever be considered. In this book, in particular, we shall study set, and sets of sets, and similar towers of sometimes frightening height and complexity — and nothing else. By way of examples we might occasionally speak of sets of cabbages, and kings, and the like, but such usage is always to be construed as an illuminating parable only, and not as a part of the theory that is being developed.\nThe principal concept of set theory, the one that in completely axiomatic studies is the principal primitive (undefined) concept, is that of belonging. If \\(x\\) belongs to \\(A\\) (\\(x\\) is an element of \\(A\\), \\(x\\) is contained in \\(A\\)), we shall write\n\\[\nx \\in A.\n\\]\nThis version of the Greek letter epsilon is so often used to denote belonging that its use to denote anything else is almost prohibited. Most authors relegate \\(\\in\\) to its set-theoretic use forever and use \\(\\varepsilon\\) when they need the fifth letter of the Greek alphabet.\nPerhaps a brief digression on alphabetic etiquette in set theory might be helpful. There is no compelling reason for using small and capital letters as in the preceding paragraph; we might have written, and often will write, things like \\(x \\in y\\) and \\(A \\in B\\). Whenever possible, however, we shall informally indicate the status of a set in a particular hierarchy under consideration by means of the convention that letters at the beginning of the alphabet denote elements, and letters at the end denote sets containing them; similarly letters of a relatively simple kind denote elements, and letters of the larger and gaudier fonts denote sets containing them. Examples: \\(x \\in A\\), \\(A \\in X\\), \\(X \\in \\mathcal{C}\\).\nA possible relation between sets, more elementary than belonging, is equality. The equality of two sets \\(A\\) and \\(B\\) is universally denoted by the familiar symbol\n\\[\nA = B;\n\\]\nthe fact that \\(A\\) and \\(B\\) are not equal is expressed by writing\n\\[\nA \\neq B.\n\\]\nThe most basic property of belonging is its relation to equality, which can be formulated as follows.\n\nAxiom 1.1 (Axiom of extension) Two sets are equal if and only if they have the same elements.\n\nWith greater pretentiousness and less clarity: a set is determined by its extension.\nIt is valuable to understand that the axiom of extension is not just a logically necessary property of equality but a non-trivial statement about belonging. One way to come to understand the point is to consider a partially analogous situation in which the analogue of the axiom of extension does not hold. Suppose, for instance, that we consider human beings instead of sets, and that, if \\(x\\) and \\(A\\) are human beings, we write \\(x \\in A\\) whenever \\(x\\) is an ancestor of \\(A\\). (The ancestors of a human being are his parents, his parents’ parents, their parents, etc., etc.) The analogue of the axiom of extension would say here that if two human beings are equal, then they have the same ancestors (this is the “only if” part, and it is true), and also that if two human being the same ancestors, then they are equal (this is the “if” part, and it is false).\nIf \\(A\\) and \\(B\\) are sets and if every element of \\(A\\) is an element of \\(B\\), we say that \\(A\\) is a subset of \\(B\\), or \\(B\\) includes \\(A\\), and we write\n\\[\nA \\subset B\n\\]\nor\n\\[\nA \\supset B.\n\\]\nThe wording of the definition implies that each set must be considered to be included in itself (\\(A \\subset A\\)); this fact is described by saying that set inclusion is reflexive. (Note that; in the same sense of the word, equality also is reflexive.) If \\(A\\) and \\(B\\) are sets such that \\(A \\subset B\\) and \\(A \\neq B\\), the word proper is used (proper subset, proper inclusion). If \\(A\\), \\(B\\), and \\(C\\) are sets such that \\(A \\subset B\\) and \\(B \\subset C\\), then \\(A \\subset C\\); this fact is described by saying that set inclusion is transitive. (This property is also shared by equality.)\nIf \\(A\\) and \\(B\\) are sets such that \\(A \\subset B\\) and \\(B \\subset A\\), then \\(A\\) and \\(B\\) have the same elements and therefore, by the axiom of extension, \\(A = B\\). This fact is described by saying that set inclusion is antisymmetric. (In this respect set inclusion behaves differently from equality. Equality is symmetric, in the sense that if \\(A = B\\), then necessarily \\(B = A\\).) The axiom of extension can, in fact, be reformulated in these terms: if \\(A\\) and \\(B\\) are sets, then a necessary and sufficient condition that \\(A = B\\) is that both \\(A \\subset B\\) and \\(B \\subset A\\). Correspondingly , almost all proofs of equalities between two sets \\(A\\) and \\(B\\) are split into two parts; first show that \\(A \\subset B\\), and then show that \\(B \\subset A\\).\nObserve that belonging (\\(\\in\\)) and inclusion (\\(\\subset\\)) are conceptually very different indeed. One important difference has already manifested itself above: inclusion is always reflexive, whereas it is not at all clear that belonging is ever reflexive. That is: \\(A \\subset A\\) is always true; is \\(A \\in A\\) ever true? It is certainly not true of any reasonable set that anyone has ever seen. Observe, along the same lines, that inclusion is transitive, whereas belonging is not. Everyday examples, involving, for instance, super-organizations whose members are organizations, will readily occur to the interested reader.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Axiom of Extension</span>"
    ]
  },
  {
    "objectID": "chapter_2.html",
    "href": "chapter_2.html",
    "title": "2  The Axiom of Specification",
    "section": "",
    "text": "All the basic principles of set theory, except only the axiom of extension, are designed to make new sets out of old ones. The first and most important of these basic principles of set manufacture says, roughly speaking, that anything intelligent one can assert about the elements of a set specifies a subset, namely, the subset of those elements about which the assertion is true.\nBefore formulating this principle in exact terms, we look at a heuristic example. Let \\(A\\) be the set of all men. The sentence “\\(x\\) is married” is true for some of the elements \\(x\\) of \\(A\\) and false for others. The principle we are illustrating is the one that justifies the passage from the given set \\(A\\) to the subset (namely, the set of all married men) specified by the given sentence. To indicate the generation of the subset, it is usually denoted by\n\\[\n\\{ x \\in A: x \\mathit{\\ is\\ married} \\}.\n\\]\nSimilarly\n\\[\n\\{ x \\in A: x \\mathit{\\ is\\ not\\ married} \\}\n\\]\nis the get of all bachelors;\n\\[\n\\{ x \\in A: \\mathit{\\ the\\ father\\ of\\ x\\ is\\ Adam} \\}\n\\]\nis the set that contains Seth, Cain and Abel and nothing else; and\n\\[\n\\{ x \\in A: x \\mathit{\\ is\\ the\\ father\\ of\\ Abel} \\}  \n\\]\nis the set that contains Adam and nothing else. Warning: a box that contains a hat and nothing else is not the same thing as a hat, and, in the same way, the last set in this list of examples is not to be confused with Adam. The analogy between sets and boxes has many weak points, but sometimes it gives a helpful picture of the facts.\nAll that is lacking for the precise general formulation that underlies the examples above is a definition of sentence . Here is a quick and informal one. There are two basic types of sentences, namely, assertions of belonging,\n\\[\nx \\in A,\n\\]\nand assertions of equality,\n\\[\nA = B;\n\\]\nall other sentences are obtained from such atomic sentences by repeated applications of the usual logical operators, subject only to the minimal courtesies of grammar and unambiguity. To make the definition more explicit (and longer) it is necessary to append to it a list of the “usual logical operators” and the rules of syntax. An adequate (and, in fact, redundant) list of the former contains seven items:\n        and,\n        or (in the sense of “either — or — or both”),\n        not,\n        if—then—(or implies),\n        if and only if,\n        for some (or there exists),\n        for all.\nAs for the rules of sentence construction, they can be described as follows. (i) Put “not” before a sentence and enclose the result between parentheses. (The reason for parentheses, here and below, is to guarantee unambiguity. Note, incidentally, that they make all other punctuation marks unnecessary. The complete parenthetical equipment that the definition of sentences calls for is rarely needed. We shall always omit as many parentheses as it seems safe to omit without leading to confusion. In normal mathematical practice, to be followed in this book, several different sizes and shapes of parentheses are used, but that is for visual convenience only.) (ii) Put “and” or “or” or “if and only if” between two sentences and enclose the result between parentheses. (iii) Replace the dashes in “if—then—” by sentences and enclose the result in parentheses. (iv) Replace the dash in “for some—” or in “for all—” by a letter, follow the result by a sentence, and enclose the whole in parentheses. (If the letter used does not occur in the sentence, no harm is done. According to the usual and natural convention “for some \\(y\\ (x \\in A)\\)” just means “\\(x \\in A\\)”. It is equally harmless if the letter used has already been used with “for some—.” Recall that “for some \\(x\\ (x \\in A)\\)” means the same as “for some \\(y\\ (y \\in A)\\)”; it follows that a judicious change of notation will always avert alphabetic collisions.)\nWe are now ready to formulate the major principle of set theory, often referred to by its German name Aussonderungsaxiom.\n\nAxiom 2.1 (Axiom of specification) To every set \\(A\\) and to every condition \\(S(x)\\) corresponds a set \\(B\\) whose elements are exactly those elements \\(x\\) of \\(A\\) for which \\(S(x)\\) holds.\n\nA “condition” here is just a sentence. The symbolism is intended to indicate the letter \\(x\\) is free in the sentence \\(S(x)\\); that means that \\(x\\) occurs in \\(S(x)\\) at least once without being introduced by one of the phrases “for some \\(x\\)” or “for all \\(x\\)”. It is an immediate consequence of the axiom of extension that the axiom of specification determines the set \\(B\\) uniquely. To indicate the way \\(B\\) is obtained from \\(A\\) and from \\(S(x)\\) it is customary to write\n\\[\nB  = \\{ x \\in A: S(x) \\}.\n\\]\nTo obtain an amusing and instructive application of the axiom of specification, consider, in the role of \\(S(x)\\), the sentence\n\\[\n\\text{not } (x \\in x).\n\\]\nIt will be convenient, here and throughout, to write “\\(x \\notin  A\\)” instead of “not \\((x \\in A)\\)”; in this notation, the role of \\(S(x)\\) is now played by\n\\[\nx \\notin x.\n\\]\nIt follows that, whatever the set \\(A\\) may be, if \\(B = {x \\in A: x \\notin x}\\), then, for all \\(y\\),\n\\[\ny \\in B \\mathit{\\ if\\ and\\ only\\ if\\ } ( y \\in A \\mathit{\\ and\\ } y \\notin y).\n\\tag{2.1}\\]\nCan it be that \\(B \\in A\\)? We proceed to prove that the answer is no. Indeed, if \\(B \\in A\\), then either \\(B \\in B\\) also (unlikely, but not obviously impossible), or else \\(B \\notin B\\). If \\(B \\in B\\), then, by Equation 2.1, the assumption \\(B \\in A\\) yields \\(B \\notin B\\)—a contradiction. If \\(B \\notin B\\), then, by Equation 2.1 again, the assumption \\(B \\in A\\) yields \\(B \\in B\\)—a contradiction again. This completes the proof that is impossible, so that we must have \\(B \\notin A\\). The most interesting part of this conclusion is that there exists something (namely \\(B\\)) that does not belong to \\(A\\). The set \\(A\\) in this argument was quite arbitrary. We have proved, in other words, that\n\\[\n\\mathit{nothing\\ contains\\ everything,}\n\\]\nor, more spectacularly,\n\\[\n\\mathit{there\\ is\\ no\\ universe.}\n\\]\n“Universe” here is used in the sense of “universe of discourse,” meaning, in any particular discussion, a set that contains all the objects that enter into that discussion.\nIn older (pre-axiomatic) approaches to set theory, the existence of universe was taken for granted, and the argument in the preceding paragraph was known as the Russell’s paradox. The moral is that it is impossible, especially in mathematics, to get something for nothing. To specify a set, it is not enough to pronounce some magic words (which may form a sentence such as “\\(x \\notin x\\)”); it is necessary also to have at hand a set to whose elements the magic words apply.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Axiom of Specification</span>"
    ]
  },
  {
    "objectID": "chapter_3.html",
    "href": "chapter_3.html",
    "title": "3  Unordered Pairs",
    "section": "",
    "text": "For all that has been said so far, we might have been operating in a vacuum. To give the discussion some substance, let us now officially assume that\n\\[\n\\mathit{there\\ exists\\ a\\ set}.\n\\]\nSince later on we shall formulate a deeper and more useful existential assumption, this assumption plays a temporary role only. One consequence of this innocuous seeming assumption is that there exists a set without any elements at all. Indeed, if \\(A\\) is a set, apply the axiom of specification to \\(A\\) with the sentence “\\(x \\neq x\\)” (or, for that matter, with any other universally false sentence). The result is the set \\(\\{x \\in A: x \\neq x\\}\\), and that set, clearly, has no elements. The axiom of extension implies that there can be only one set with no elements. The usual symbol for that set is\n\\[\n\\emptyset;\n\\]\nthe set is called the empty set.\nThe empty set is a subset of every set, or, in other words, \\(\\emptyset \\subset A\\) for every \\(A\\). To establish this, we might argue as follows. It is to be proved that every element in \\(\\emptyset\\) belongs to \\(A\\); since there are no elements in \\(\\emptyset\\), the condition is automatically fulfilled. The reasoning is correct but perhaps unsatisfying. Since it is a typical example of a frequent phenomenon, a condition holding in the “vacuous” sense, a word of advice to the inexperienced reader might be in order. To prove that something is true about the empty set, prove that it cannot be false. How, for instance, could it be false that \\(\\emptyset \\subset A\\)? It could be false only if \\(\\emptyset\\) had an element that did not belong to \\(A\\). Since \\(\\emptyset\\) has no elements at all, this is absurd. Conclusion: \\(\\emptyset \\subset A\\) is not false, and therefore \\(\\emptyset \\subset A\\) for every \\(A\\).\nThe set theory developed so far is still a pretty poor thing; for all we know there is only one set and that one is empty. Are there enough sets to ensure that every set is an element of some set? Is it true that for any two sets there is a third one that they both belong to? What about three sets, or four, or any number? We need a new principle of set construction to resolve such questions. The following principle is a good beginning.\n\nAxiom 3.1 (Axiom of pairing) For any two sets there exists a set that they both belong to.\n\nNote that this is just the affirmative answer to the second question above.\nTo reassure worriers, let us hasten to observe that words such as “two,” “three,” and “four,” used above, do not refer to the mathematical concepts bearing those names, which will be defined later; at present such words are merely the ordinary linguistic abbreviations for “something and then something else” repeated an appropriate number of times. Thus, for instance, the axiom of pairing, in unabbreviated form, says that if \\(a\\) and \\(b\\) are sets, then there exists a set \\(A\\) such that \\(a \\in A\\) and \\(b \\in A\\).\nOne consequence (in fact an equivalent formulation) of the axiom of pairing is that for any two sets there exists a set that contains both of them and nothing else. Indeed, if \\(a\\) and \\(b\\) are sets, and if \\(A\\) is a set such that \\(a \\in A\\) and \\(b \\in A\\), then we can apply the axiom of specification to \\(A\\) with the sentence “\\(x = a \\mathit{\\ or\\ } x = b\\).” The result is the set\n\\[\n\\{ x \\in A: x = a \\mathit{\\ or\\ } x = b \\},\n\\]\nand that set, clearly, contains just \\(a\\) and \\(b\\). The axiom of extension implies that there can be only one set with this property. The usual symbol for that set is\n\\[\n\\{ a, b \\};\n\\]\nthe set is called the pair (or, by way of emphatic comparison with a subsequent concept, the unordered pair) formed by \\(a\\) and \\(b\\).\nIf, temporarily, we refer to the sentence “\\(x = a \\mathit{\\ or\\ } x = b\\)” as \\(S(x)\\), we may express the axiom of pairing by saying that there exists a set \\(B\\) such that\n\\[\nx \\in B \\mathit{\\ if\\ and\\ only\\ if\\ } S(x).\n\\tag{3.1}\\]\nThe axiom of specification, applied to a set \\(A\\), asserts the existence of a set \\(B\\) such that\n\\[\nx \\in B \\mathit{\\ if\\ and\\ only\\ if\\ } (x \\in A \\mathit{\\ and\\ } S(x)).\n\\tag{3.2}\\]\nThe relation between Equation 3.1 and Equation 3.2 typifies something that occurs quite frequently. All the remaining principles of set construction are pseudo-special cases of the axiom of specification in the sense in which Equation 3.1 is a pseudo-special case of Equation 3.2. They all assert the existence of a set specified by a certain condition; if it were known in advance that there exists a set containing all the specified elements, then the existence of a set containing just them would indeed follow as a special case of the axiom of specification.\nIf \\(a\\) is a set, we may form the unordered pairs \\(\\{a, a\\}\\). That unordered pair is denoted by\n\\[\n\\{ a \\}\n\\]\nand is called the singleton of \\(a\\); it is uniquely characterized by the statement that it has \\(a\\) as its only element. Thus, for instance, \\(\\emptyset\\) and \\(\\{ \\emptyset \\}\\) are very different sets; the former has no elements, whereas the latter has the unique element \\(\\emptyset\\). To say that \\(a \\in A\\) is equivalent to saying that \\(\\{a\\} \\subset A\\).\nThe axiom of pairing ensures that every set is an element of some set and that any two sets are simultaneously elements of some one and the same set. (The corresponding questions for three and four and more sets will be answered later.) Another pertinent comment is that from the assumptions we have made so far we can infer the existence of very many sets indeed. For examples consider the sets \\(\\emptyset, \\{ \\emptyset \\}, \\{ \\{ \\emptyset \\} \\}, \\{ \\{ \\{ \\emptyset \\} \\} \\}\\), etc.; consider the pairs, such as \\(\\{ \\emptyset, \\{ \\emptyset \\} \\}\\), formed by any two of them; consider the pairs formed by any two such pairs, or else the mixed pairs formed by any singleton and any pair; proceed so on ad infinitum.\n\nExercise 3.1 Are all the sets obtained in this way distinct from one another?\n\nBefore continuing our study of set theory, we pause for a moment to discuss a notational matter. It seems natural to denote the set \\(B\\) described in Equation 3.1 by \\(\\{x: S(x)\\}\\); in the special case that was there considered\n\\[\n\\{ x : x = a \\mathit{\\ or\\ } x = b \\} = \\{ a,b \\}.\n\\]\nWe shall use this symbolism whenever it is convenient and permissible to do so. If, that is, \\(S(x)\\) is a condition on \\(x\\) such that the \\(x\\)’s that \\(S(x)\\) specifies constitute a set, then we may denote that set by\n\\[\n\\{ x : S(x) \\}.\n\\]\nIn case \\(A\\) is a set and \\(S(x)\\) is \\((x \\in A)\\), then it is permissible to form \\(\\{x: S(x)\\}\\); in fact\n\\[\n\\{ x : x \\in A \\} = A.\n\\]\nIf \\(A\\) is a set and \\(S(x)\\) is an arbitrary sentence, it is permissible to form \\(\\{ x : x \\in A \\mathit{\\ and\\ } S(x) \\}\\); this set is the same as \\(\\{x \\in A: S(x) \\}\\). As further examples, we note that\n\\[\n\\{ x : x \\neq x \\} = \\emptyset\n\\]\nand\n\\[\n\\{ x : x = a \\} = \\{ a \\}.\n\\]\nIn case \\(S(x)\\) is \\((x \\notin x)\\), or in case \\(S(x)\\) is \\((x = x)\\), the specified \\(x\\)’s do not constitute a set.\nDespite the maxim about never getting something for nothing, it seems a little harsh to be told that certain sets are not really sets and even their names must never be mentioned. Some approaches to set theory try to soften the blow by making systematic use of such illegal sets but just not calling them sets; the customary word is “class”. A precise explanation of what classes really are and how they are used is irrelevant in the present approach. Roughly speaking, a class may be identified with a condition (sentence), or, rather, with the “extension” of a condition.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Unordered Pairs</span>"
    ]
  },
  {
    "objectID": "chapter_4.html",
    "href": "chapter_4.html",
    "title": "4  Unions and Intersections",
    "section": "",
    "text": "If \\(A\\) and \\(B\\) are sets, it is sometimes natural to wish to unite their elements into one comprehensive set. One way of describing such a comprehensive set is to require it to contain all the elements that belong to at least one of the two members of the pair \\(\\{ A, B \\}\\). This formulation suggests a sweeping generalization of itself; surely a similar construction should apply to arbitrary collections of sets and not just to pairs of them. What is wanted, in other words, is the following principle of set construction.\n\nAxiom 4.1 (Axiom of unions) For every collection of sets there exists a set that contains all the elements that belong to at least one set of the given collection.\n\nHere it is again: for every collection \\(\\mathcal{C}\\) there exists a set \\(U\\) such that if \\(x \\in X\\) for some \\(X\\) in \\(\\mathcal{C}\\), then \\(x \\in U\\). (Note that “at least one” is the same as “some.”)\nThe comprehensive set \\(U\\) described above may be too comprehensive; it may contain elements that belong to none of the sets \\(X\\) in the collection \\(\\mathcal{C}\\). This is easy to remedy; just apply the axiom of specification to form the set\n\\[\n\\{ x \\in U: x \\in X \\mathit{\\ for\\ some\\ } X \\mathit{\\ in\\ } \\mathcal{C} \\}.\n\\]\n(The condition here is a translation into idiomatic usage of the mathematically more acceptable “for some \\(X\\ (x \\in X \\mathit{\\ and\\ } X \\in \\mathcal{C})\\).”) It follows that, for every \\(x\\), a necessary and sufficient condition that \\(x\\) belong to this set is that \\(x\\) belong to \\(X\\) for some \\(X\\) in \\(\\mathcal{C}\\). If we change notation and call the new set \\(U\\) again, then\n\\[\nU = \\{ x: x \\in X \\mathit{\\ for\\ some\\ } X \\mathit{\\ in\\ }  \\mathcal{C} \\}.\n\\]\nThis set \\(U\\) is called the union of the collection \\(\\mathcal{C}\\) of sets; note that the axiom of extension guarantees its uniqueness. The simplest symbol for \\(U\\) that is in use at all is not very popular in mathematical circles; it is\n\\[\n\\bigcup \\mathcal{C}.\n\\]\nMost mathematicians prefer something like\n\\[\n\\bigcup \\{ X: X \\in \\mathcal{C} \\}\n\\]\nor\n\\[\n\\bigcup_{X \\in \\mathcal{C}} X.\n\\]\nFurther alternatives are available in certain important special cases; they will be described in due course.\nFor the time being we restrict our study of the theory of unions to the simplest facts only. The simplest fact of all is that\n\\[\n\\bigcup \\{ X: X \\in \\emptyset \\} = \\emptyset,\n\\]\nand the next simplest fact is that\n\\[\n\\bigcup \\{ X: X \\in \\{ A \\} \\} = A.\n\\]\nIn the brutally simple notation mentioned above these facts are expressed by\n\\[\n\\bigcup \\emptyset = \\emptyset\n\\]\nand\n\\[\n\\bigcup \\{ A \\} = A.\n\\]\nThe proofs are immediate from the definitions.\nThere is a little more substance in the union of pairs of sets (which is what started this whole discussion anyway). In that case special notation is used:\n\\[\n\\bigcup \\{ X: X \\in \\{A, B \\} \\} = A \\cup B.\n\\]\nThe general definition of unions implies in the special case that \\(x \\in A \\cup B\\) if and only if \\(x\\) belongs to either \\(A\\) or \\(B\\) or both; it follows that\n\\[\nA \\cup B = \\{x: x \\in A \\mathit{\\ or\\ } x \\in B \\}.\n\\]\nHere are some easily proved facts about the unions of pairs:\n\\[\nA \\cup \\emptyset = A,\n\\] \\[\nA \\cup B =  B \\cup A \\ (commutatitivity),\n\\] \\[\nA \\cup (B \\cup C) = (A \\cup B) \\cup C \\ (associativity),\n\\] \\[\nA \\cup A = A \\ (idempotence),\n\\] \\[\nA \\subset B \\mathit{\\ if\\ and\\ only\\ if\\ } A \\cup B = B.\n\\]\nEvery student of mathematics should prove these things for himself at least once in his life. The proofs are based on the corresponding elementary properties of the logical operator or.\nAn equally simple but quite suggestive fact is that\n\\[\n\\{ a \\} \\cup \\{ b \\} = \\{ a, b \\}.\n\\]\nWhat this suggests is the way to generalize pairs. Specifically, we write\n\\[\n\\{ a, b, c \\} = \\{ a \\} \\cup \\{ b \\} \\cup \\{ c \\}.\n\\]\nThe equation defines its left side. The right side should by rights have at least one pair of parentheses in it, but, in view of the associative law, their omission can lead to no misunderstanding. Since it is easy to prove that\n\\[\n\\{ a, b, c \\} = \\{ x: x = a \\ or \\ x = b \\: or \\: x = c \\},\n\\]\nwe know now that for every three sets there exists a set that contains them and nothing else; it is natural to call that uniquely determined set the (unordered) triple formed by them. The extension of the notation and terminology thus introduced to more terms (quadruples, etc.) is obvious.\nThe formation of unions has many points of similarity with another set-theoretic operation. If \\(A\\) and \\(B\\) are sets, the intersection of \\(A\\) and \\(B\\) is the set\n\\[\nA \\cap B\n\\]\ndefined by\n\\[\nA \\cap B = \\{ x \\in A: x \\in B \\}.\n\\]\nThe definition is symmetric in \\(A\\) and \\(B\\) even if it looks otherwise; we have\n\\[\nA \\cap B = \\{ x \\in B: x \\in A \\},\n\\]\nand, in fact, since \\(x \\in A \\cap B\\) if and only if \\(x\\) belongs to both \\(A\\) and \\(B\\), it follows that\n\\[\nA \\cap B = \\{x: x \\in A \\mathit{\\ and\\ } x \\in B \\}.\n\\]\nThe basic facts about intersections, as well as their proofs, are similar to the basic facts about unions:\n\\[\nA \\cap \\emptyset = \\emptyset,\n\\] \\[\nA \\cap B =  B \\cap A,\n\\] \\[\nA \\cap (B \\cap C) = (A \\cap B) \\cap C,\n\\] \\[\nA \\cap A = A,\n\\] \\[\nA \\subset B \\mathit{\\ if\\ and\\ only\\ if\\ } A \\cap B = A.\n\\]\nPairs of sets with an empty intersection occur frequently enough to justify the use of a special word: if \\(A \\cap B =  \\emptyset\\), the sets \\(A\\) and \\(B\\) are called disjoint. The same word is sometimes applied to a collection of sets to indicate that any two distinct sets of the collection are disjoint; alternatively we may speak in such a situation of a pairwise disjoint collection.\nTwo useful facts about unions and intersections involve both the operations at the same time:\n\\[\nA \\cap (B \\cup C) = (A \\cap B) \\cup (A \\cap C),\n\\] \\[\nA \\cup (B \\cap C) = (A \\cup B) \\cap (A \\cup C).\n\\]\nThese identities are called the distributive laws. By way of a sample of a set-theoretic proof, we prove the second one. If \\(x\\) belongs to the left side, then \\(x\\) belongs either to \\(A\\) or to both \\(B\\) and \\(C\\); if \\(x\\) is in \\(A\\), then \\(x\\) is in both \\(A \\cup B\\) and \\(A \\cup C\\), and if \\(x\\) is in both \\(B\\) and \\(C\\), then, again, \\(x\\) is in both \\(A \\cup B\\) and \\(A \\cup C\\); it follows that, in any case, \\(x\\) belongs to the right side. This proves that the right side includes the left. To prove the reverse inclusion, just observe that if \\(x\\) belongs to both \\(A \\cup B\\) and \\(A \\cup C\\), then \\(x\\) belongs either to \\(A\\) or to both \\(B\\) and \\(C\\).\nThe formation of the intersection of two sets \\(A\\) and \\(B\\), or, we might as well say, the formation of the intersection of a pair \\(\\{ A, B \\}\\) of sets, is a special case of a much more general operation. (This is another respect in which the theory of intersections imitates that of unions.) The existence of the general operation of intersection depends on the fact that for each non-empty collection of sets there exists a set that contains exactly those elements that belong to every set of the given collection. In other words: for each collection \\(\\mathcal{C}\\), other than \\(\\emptyset\\), there exists a set \\(V\\) such that \\(x \\in V\\) if and only if \\(x \\in X\\) for every \\(X\\) in \\(\\mathcal{C}\\). To prove this assertion, let \\(A\\) be any particular set in \\(\\mathcal{C}\\) (this step is justified by the fact that \\(\\mathcal{C} \\neq \\emptyset\\)) and write\n\\[\nV = \\{ x \\in A: x \\in X \\mathit{\\ for\\ every\\ } X \\mathit{\\ in\\ } \\mathcal{C} \\}.\n\\]\n(The condition means “for all \\(X\\) (if \\(X \\in \\mathcal{C}\\), then \\(x \\in X\\)).”) The dependence of \\(V\\) on the arbitrary choice of \\(A\\) is illusory; in fact\n\\[\nV = \\{ x : x \\in X \\mathit{\\ for\\ every\\ } X \\mathit{\\ in\\ } \\mathcal{C} \\}.\n\\]\nThe set \\(V\\) is called the intersection of the collection \\(\\mathcal{C}\\) of sets; the axiom of extension guarantees its uniqueness. The customary notation is similar the one for unions: instead of the unobjectionable but unpopular\n\\[\n\\bigcap \\mathcal{C},\n\\]\nthe set \\(V\\) is usually denoted by\n\\[\n\\bigcap \\{ X : X \\in \\mathcal{C} \\}\n\\]\nor\n\\[\n\\bigcap_{X \\in \\mathcal{C}} X.\n\\]\n\nExercise 4.1 A necessary and sufficient condition that \\((A \\cap B) \\cup C = A \\cap (B \\cup C)\\) is that \\(C \\subset A\\). Observe that the condition has nothing to do with the set \\(B\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Unions and Intersections</span>"
    ]
  },
  {
    "objectID": "chapter_5.html",
    "href": "chapter_5.html",
    "title": "5  Complements and Powers",
    "section": "",
    "text": "If \\(A\\) and \\(B\\) are sets, the difference between \\(A\\) and \\(B\\), more often known as the relative complement of \\(B\\) in \\(A\\), is the set \\(A - B\\) defined by\n\\[\nA - B = \\{ x \\in A: x \\notin B \\}.\n\\]\nNote that in this definition it is not necessary to assume that \\(B \\subset A\\). In order to record the basic facts about complementation as simply as possible, we assume nevertheless (in this section only) that all the sets to be mentioned are subsets of one and the same set \\(E\\) and that all complements (unless otherwise specified) are formed relative to that \\(E\\). In such situations (and they are quite common) it is easier to remember the underlying set \\(E\\) than to keep writing it down, and this makes it possible to simplify the notation. An often used symbol for the temporarily absolute (as opposed to relative) complement of \\(A\\) is \\(A'\\). In terms of this symbol the basic facts about complementation can be stated as follows:\n\\[(A')' = A,\\] \\[\\emptyset' = E, \\: E' = \\emptyset,\\] \\[A \\cap A' = \\emptyset, \\: A \\cup A' = E,\\] \\[A \\subset B \\mathit{\\ if\\ and\\ only\\ if\\ } B' \\subset A'.\\]\nThe most important statements about complements are the so-called De Morgan laws:\n\\[\n(A \\cup B)' = A' \\cap B', \\: (A \\cap B)' = A' \\cup B'.\n\\]\n(We shall see presently that the De Morgan laws hold for the unions and intersections of larger collections of sets than just pairs.) These facts about complementation imply that the theorems of set usually come in pairs. If in an inclusion equation involving unions, intersections, and complements of subsets of \\(E\\) we replace each set by its complement, interchange unions and intersections, and reverse all inclusions, the result is another theorem. This fact is sometimes referred to as the principle of duality for sets.\nHere are some easy exercises on complementation.\n\\[A - B = A \\cap B'.\\] \\[A \\subset B \\mathit{\\ if\\ and\\ only\\ if\\ } A - B = \\emptyset.\\] \\[A - (A - B) = A \\cap B.\\] \\[A \\cap (B - C) = (A \\cap B) - (A \\cap C).\\] \\[A \\cap B \\subset (A \\cap C) \\cup (B \\cap C').\\] \\[(A \\cup C) \\cap (B \\cup C') \\subset A \\cup B.\\]\nIf \\(A\\) and \\(B\\) are sets, the symmetric difference (or Boolean sum of \\(A\\) and \\(B\\) is the set \\(A + B\\) defined by\n\\[\nA + B = (A - B) \\cup (B - A).\n\\]\nThis operation is commutative \\((A + B = B + A)\\) and associative \\((A + (B + C) =  (A + B) + C)\\), and is such that \\(A + \\emptyset = A\\) and \\(A + A = \\emptyset\\).\nThis may be the right time to straighten out a trivial but occasionally puzzling part of the theory of intersections. Recall, to begin with, that intersections were defined for non-empty collections only. The reason is that the same approach to the empty collection does not define a set. Which \\(x\\)’s are specified by the sentence\n\\[\nx \\in X \\mathit{\\ for\\ every\\ } X \\mathit{\\ in\\ } \\emptyset ?\n\\]\nAs usual for questions about \\(\\emptyset\\) the answer is easier to see for the corresponding negative question. Which \\(x\\)’s do not satisfy the stated condition? If it is not true that \\(x \\in X\\) for every \\(X\\) in \\(\\emptyset\\), then there must exist an \\(X\\) in \\(\\emptyset\\) such that \\(x \\notin X\\); since, however, there do not exist any \\(X\\)’s in \\(\\emptyset\\) at all, this is absurd. Conclusion: no \\(x\\) fails to satisfy the stated condition, or, equivalently, every \\(x\\) does satisfy it. In other words, the \\(x\\)’s that the condition specifies exhaust the (nonexistent) universe. There is no profound problem here; it is merely a nuisance to be forced always to be making qualifications and exceptions just because some set somewhere along some construction might turn out to be empty. There is nothing to be done about this; it is just a fact of life.\nIf we restrict our attention to subsets of a particular set \\(E\\), as we have temporarily agreed to do, then the unpleasantness described in the preceding paragraph appears to go away. The point is that in that case we can define the intersection of a collection \\(\\mathcal{C}\\) (of subsets of \\(E\\)) to be the set\n\\[\n\\{ x \\in E: x \\in X \\mathit{\\ for\\ every\\ } X \\in \\mathcal{C} \\}.\n\\]\nThis is nothing revolutionary; for each non-empty collection, the new definition agrees with the old one. The difference is in the way the old and the new definitions treat the empty collection; according to the new definition \\(\\bigcap_{X \\in \\emptyset} X\\) is equal to \\(E\\). (For which elements \\(x\\) of \\(E\\) can it be false that \\(x \\in X\\) for every \\(X\\) in \\(\\emptyset\\)?) The difference is just a matter of language. A little reflection reveals that the “new” definition offered for intersection of a collection \\(\\mathcal{C}\\) of subsets of \\(E\\) is really the same as the old definition of the intersection of the collection \\(\\mathcal{C} \\cup \\{ E \\}\\), and the latter is never empty.\nWe have been considering the subsets of a set \\(E\\); do those subsets themselves constitute a set? The following principle guarantees that the answer is yes.\n\nAxiom 5.1 (Axiom of powers) For each set there exists a collection of sets that contains among its elements all the subsets of the given set.\n\nIn other words, if \\(E\\) is a set, then there exists a set (collection) \\(\\mathcal{P}\\) such that if \\(X \\subset E\\), then \\(X \\in \\mathcal{P}\\).\nThe set \\(\\mathcal{P}\\) described above may be larger than wanted; it may contain elements other than subsets of \\(E\\). This is easy to remedy; just apply the axiom of specification to form the set \\(\\{ X \\in \\mathcal{P}: X \\subset E \\}\\). (Recall that “\\(X \\subset E\\)” says the same thing as “\\(\\mathit{\\ for\\ all\\ x\\ (if\\ } x \\in X \\mathit{\\ then\\ } x \\in E)\\).”) Since, for every \\(X\\), a necessary and sufficient condition that \\(X\\) belong to this set is that \\(X\\) be a subset of \\(E\\), it follows that if we change notation and call this set \\(\\mathcal{P}\\) again, then\n\\[\n\\mathcal{P} = \\{ X : X \\subset E \\}.\n\\]\nThe set \\(\\mathcal{P}\\) is called the power set of \\(E\\); the axiom of extension guarantees its uniqueness. The dependence of \\(\\mathcal{P}\\) on \\(E\\) is denoted by writing \\(\\mathcal{P}(E)\\) instead of just \\(\\mathcal{P}\\).\nBecause the set \\(\\mathcal{P}(E)\\) is very big in comparison with \\(E\\), it is not easy to give examples. If \\(E = \\emptyset\\), the situation is clear enough; the set \\(\\mathcal{P}(\\emptyset)\\) is the singleton \\(\\{ \\emptyset \\}\\). The power sets of singletons and pairs are also easily describable; we have\n\\[\n\\mathcal{P} ( \\{ a \\} ) = \\{ \\emptyset, \\{ a \\} \\}\n\\]\nand\n\\[\n\\mathcal{P} ( \\{ a, b \\} ) = \\{ \\emptyset, \\{ a \\} , \\{ b \\} , \\{ a,b \\} \\}.\n\\]\nThe power set of a triple has eight elements. The reader can probably guess (and is hereby challenged to prove) the generalization that includes all these statements: the power set of a finite set with, say, \\(n\\) elements has \\(2^n\\) elements. (Of course concepts like “finite” and “\\(2^{n}\\)” have no official standing for us yet; this should not prevent them from being unofficially understood.) The occurrence of \\(n\\) as an exponent (the \\(n\\)-th power of 2) has something to do with the reason why power set bears its name.\nIf \\(\\mathcal{C}\\) is a collection of subsets of a set \\(E\\) (that is, \\(\\mathcal{C}\\) is a subcollection of \\(\\mathcal{P}(E)\\)), then write\n\\[\n\\mathcal{D} = \\{ X \\in \\mathcal{P}(E): X' \\in \\mathcal{C} \\}.\n\\]\n(To be certain that the condition used in the definition of \\(\\mathcal{D}\\) is a sentence in the precise technical sense, it must be rewritten in something like the form\n\\[\n\\mathit{for\\ some\\ Y\\ } [Y \\in \\mathcal{C} \\mathit{\\ and\\ for\\ all\\ x\\ } ( x \\in X \\mathit{\\ if\\ and\\ only\\ if\\ } (x \\in E \\mathit{\\ and\\ } x \\notin Y))].\n\\]\nSimilar comments often apply when we wish to use defined abbreviations instead of logical and set-theoretic primitives only. The translation rarely requires any ingenuity and we shall usually omit it.) It is customary to denote the union and the intersection of the collection \\(\\mathcal{D}\\) by the symbols\n\\[\n\\bigcup_{X \\in \\mathcal{C}} X' \\mathit{\\ and\\ } \\bigcap_{X \\in \\mathcal{C}} X'.\n\\]\nIn this notation the general forms of the De Morgan laws become\n\\[\n\\left( \\bigcup_{X \\in \\mathcal{C}} X \\right)' = \\bigcap_{X \\in \\mathcal{C}} X' .\n\\]\nand\n\\[\n\\left( \\bigcap_{X \\in \\mathcal{C}} X \\right)' = \\bigcup_{X \\in \\mathcal{C}} X'.\n\\]\nThe proofs of these equations are immediate consequences of the appropriate definitions.\n\nExercise 5.1 Prove that \\(\\mathcal{P}(E) \\cap \\mathcal{P}(F) = \\mathcal{P}(E \\cap F)\\) and \\(\\mathcal{P}(E) \\cup \\mathcal{P}(F) \\subset \\mathcal{P}(E \\cup F)\\). These assertions can be generalized to\n\\[\n\\bigcap_{X \\in \\mathcal{C}} \\mathcal{P} (X) = \\mathcal{P} \\left( \\bigcap_{X \\in \\mathcal{C}} X \\right)\n\\]\nand\n\\[\n\\bigcup_{X \\in \\mathcal{C}} \\mathcal{P}(X) \\subset \\mathcal{P} \\left( \\bigcup_{X \\in \\mathcal{C}} X \\right);\n\\]\nfind a reasonable interpretation of the notation in which these generalizations were here expressed and then prove them. Further elementary facts:\n\\[\n\\bigcap_{X \\in \\mathcal{P}(E)} X = \\emptyset,\n\\]\nand\n\\[\n\\mathit{if\\ } E \\subset F, \\mathit{\\ then\\ } \\mathcal{P}(E) \\subset \\mathcal{P}(F).\n\\]\nA curious question concerns the commutativity of the operators \\(\\mathcal{P}\\) and \\(\\bigcup\\). Show that \\(E\\) is always equal \\(\\bigcup_{X \\in \\mathcal{P}(E)} X\\) (that is \\(E = \\bigcup \\mathcal{P}(E)\\)), but that the result of applying \\(\\mathcal{P}\\) and \\(\\bigcup\\) to \\(E\\) in the other order is a set that includes \\(E\\) as a subset, typically a proper subset.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Complements and Powers</span>"
    ]
  },
  {
    "objectID": "chapter_6.html",
    "href": "chapter_6.html",
    "title": "6  Ordered Pairs",
    "section": "",
    "text": "What does it mean to arrange the elements of a set \\(A\\) in some order? Suppose, for instance, that the set \\(A\\) is the quadruple \\(\\{a, b, c, d \\}\\) of distinct elements, and suppose that we want to consider its elements in the order\n\\[\n\\mathit{c\\ b\\ d\\ a}.\n\\] Even without a precise definition of what this means, we can do something set-theoretically intelligent with it. We can, namely, consider, for each particular spot in the ordering, the set of all those elements that occur at or before that spot; we obtain in this way the sets\n\\[\n\\{ c \\} \\: \\{ c, b \\} \\: \\{ c, b, d \\} \\: \\{ c, b, d, a \\}.\n\\]\nWe can go on then to consider the set (or collection, if that sounds better)\n\\[\n\\mathcal{C} = \\{ \\{ a, b, c, d \\}, \\{ b, c \\}, \\{ b, c, d \\}, \\{ c \\} \\}\n\\]\nthat has exactly those sets for its elements. In order to emphasize that the intuitively based and possibly unclear concept of order has succeeded in producing something solid and simple, namely a plain, unembellished set \\(\\mathcal{C}\\), the elements of \\(\\mathcal{C}\\), and their elements, are presented above in a scrambled manner. (The lexicographically inclined reader might be able to see a method in the manner of scrambling.)\nLet us continue to pretend for a while that we do know what order means. Suppose that in a hasty glance at the preceding paragraph all we could catch is the set \\(\\mathcal{C}\\); can we use it to recapture the order that gave rise to it? The answer is easily seen to be yes. Examine the elements of \\(\\mathcal{C}\\) (they themselves are sets, of course) to find one that is included in all the others; since \\(\\{ c \\}\\) fills the bill (and nothing else does) we know that \\(c\\) must have been the first element. Look next for the next smallest element of \\(\\mathcal{C}\\), i.e., the one that is included in all the ones that remain after \\(\\{ c \\}\\) is removed; since \\(\\{ b, c \\}\\) fills the bill (and nothing else does), we know that \\(b\\) must have been the second element. Proceeding thus (only two more steps are needed) we pass from the set \\(\\mathcal{C}\\) to the given ordering of the given set \\(A\\).\nThe moral is this: we may not know precisely what it means to order the elements of a set \\(A\\), but with each order we can associate a set \\(\\mathcal{C}\\) of subsets of \\(A\\) in such a way that the given order can be uniquely recaptured from \\(\\mathcal{C}\\). (Here is a non-trivial exercise: find an intrinsic characterization of those sets of subsets of \\(A\\) that correspond to some order in \\(A\\). Since “order” has no official meaning for us yet, the whole problem is officially meaningless. Nothing that follows depends on the solution, but the reader would learn something valuable by trying to find it.) The passage from an order in \\(A\\) to the set \\(\\mathcal{C}\\), and back, was illustrated above for a quadruple; for a pair everything becomes at least twice as simple. If \\(A = \\{ a,b \\}\\) and if, in the desired order, \\(a\\) comes first, then \\(\\mathcal{C} = \\{ \\{ a \\} , \\{ a,b \\} \\}\\); if, however, \\(b\\) comes first, then \\(\\mathcal{C} = \\{ \\{ b \\}, \\{a, b \\}\\}\\).\nThe ordered pair of \\(a\\) and \\(b\\), with first coordinate \\(a\\) and second coordinate \\(b\\), is the set \\((a, b)\\) defined by\n\\[\n(a, b) = \\{ \\{a \\}, \\{ a, b \\} \\}.\n\\]\nHowever convincing the motivation of this definition may be, we must still prove that the result has the main property that an ordered pair must have to deserve its name. We must show that if \\((a,b)\\) and \\((x, y)\\) are ordered pairs and if \\((a,b) = (x,y)\\), then \\(a = x\\) and \\(b = y\\). To prove this, we note first that if \\(a\\) and \\(b\\) happen to be equal, then the ordered pair \\((a,b)\\) is the same as the singleton \\(\\{ \\{ a \\} \\}\\). If, conversely, \\((a,b)\\) is a singleton, then \\(\\{ a \\} = \\{ a, b \\}\\), so that \\(b \\in \\{ a \\}\\), and therefore \\(a = b\\). Suppose now that \\((a, b) = (x, y)\\). If \\(a = b\\), then both \\((a, b)\\) and \\((x,y)\\) are singletons, so that \\(x = y\\); since \\(\\{ x \\} \\in (a,b)\\) and \\(\\{ a \\} \\in (x,y)\\), it follows that \\(a, b, x,\\) and \\(y\\) are all equal. If \\(a \\neq b\\), then both \\((a, b)\\) and \\((x, y)\\) contain exactly one singleton, namely \\(\\{ a \\}\\) and \\(\\{ x \\}\\) respectively, so that \\(a = x\\). Since in this case it is also true that both \\((a,b)\\) and \\((x, y)\\) contain exactly one unordered pair that is not a singleton, namely \\(\\{ a, b \\}\\) and \\(\\{ x, y \\}\\) respectively, it follows that \\(\\{ a, b \\} = \\{ x, y \\}\\), and therefore, in particular, \\(b \\in \\{ x, y \\}\\). Since \\(b\\) cannot be \\(x\\) (for then we should have \\(a = x\\) and \\(b = x\\), and, therefore, \\(a = b\\)), we must have \\(b = y\\), and the proof is complete.\nIf \\(A\\) and \\(B\\) are sets, does there exist a set that contains all the ordered pairs \\((a,b)\\) with \\(a\\) in \\(A\\) and \\(b\\) in \\(B\\)? It is quite easy to see that the answer is yes. Indeed, if \\(a \\in A\\) and \\(b \\in B\\), then \\(\\{ a \\} \\subset A\\) and \\(\\{ b \\}  \\subset B\\), and therefore \\(\\{ a, b \\} \\subset A \\cup B\\). Since also \\(\\{ a \\} \\subset A \\cup B\\), it follows that both \\(\\{ a \\}\\) and \\(\\{ a, b \\}\\) are elements of \\(\\mathcal{P}(A \\cup B)\\). This implies that \\(\\{ \\{ a \\}, \\{ a,b \\} \\}\\) is a subset of \\(\\mathcal{P}(A \\cup B)\\), and hence that it is an element of \\(\\mathcal{P}(\\mathcal{P} (A \\cup B))\\); in other words \\((a, b) \\in \\mathcal{P}(\\mathcal{P}(A \\cup B))\\) whenever \\(a \\in  A\\) and \\(b \\in B\\). Once this is known, it is a routine matter to apply the axiom of specification and the axiom of extension to produce the unique set \\(A \\times B\\) that consists exactly of the ordered pairs \\((a, b)\\) with \\(a\\) in \\(A\\) and \\(b\\) in \\(B\\). This set is called the Cartesian product of \\(A\\) and \\(B\\); it is characterized by the fact that\n\\[\nA \\times B = \\{ x: x = (a,b) \\mathit{\\ for\\ some\\ } a \\mathit{\\ in\\ } A \\mathit{\\ and\\ for\\ some\\ } b \\mathit{\\ in\\ } B \\}.\n\\]\nThe Cartesian product of two sets is a set of ordered pairs (that is, a set each of whose elements is an ordered pair), and the same is true of every subset of a Cartesian product. It is of technical importance to know that we can go in the converse direction also: every set of ordered pairs is a subset of the Cartesian product of two sets. In other words: if \\(R\\) is a set such that every element of \\(R\\) is an ordered pair, then there exist two sets \\(A\\) and \\(B\\) such that \\(R \\subset A \\times B\\). The proof is elementary. Suppose indeed that \\(x \\in R\\), so that \\(x = \\{ \\{ a \\}, \\{ a, b \\} \\}\\) for some \\(a\\) and for some \\(b\\). The problem is to dig out \\(a\\) and \\(b\\) from under the braces. Since the elements of \\(R\\) are sets, we can form the union of the sets in \\(R\\); since \\(x\\) is one of the sets in \\(R\\), the elements of \\(x\\) belong to that union. Since \\(\\{ a, b \\}\\) is one of the elements of \\(x\\), we may write, in what has been called the brutal notation above, \\(\\{ a, b \\} \\in \\bigcup R\\). One set of braces has disappeared; let us do the same thing again to make the other set go away. Form the union of the sets in \\(\\bigcup R\\). Since \\(\\{ a, b \\}\\) is one of those sets, it follows that the elements of \\(\\{ a, b \\}\\) belong to that union, and hence both \\(a\\) and \\(b\\) belong to \\(\\bigcup \\bigcup R\\). This fulfills the promise made above; to exhibit \\(R\\) as a subset of some \\(A \\times B\\), we may take both \\(A\\) and \\(B\\) to be \\(\\bigcup \\bigcup R\\). It is often desirable to take \\(A\\) and \\(B\\) as small as possible. To do so, just apply the axiom of specification to produce the sets\n\\[\nA = \\{ a : \\mathit{\\ for\\ some\\ } b \\: ((a,b) \\in R) \\}\n\\]\nand\n\\[\nB = \\{ b : \\mathit{\\ for\\ some\\ } a \\: ((a,b) \\in R) \\}.\n\\]\nThese sets are called the projections of \\(R\\) onto the first and second coordinates respectively.\nHowever important set theory may be now, when it began some scholars considered it a disease from which, it was to be hoped, mathematics would soon recover. For this reason many set-theoretic considerations were called pathological, and the word lives on in mathematical usage; it often refers to something the speaker does not like. The explicit definition of an ordered pair \\(((a,b) = \\{ \\{ a \\}, \\{ a, b \\} \\})\\) is frequently relegated to pathological set theory. For the benefit of those who think that in this case the name is deserved, we note that the definition has served its purpose by now and will never be used again. We need to know that ordered pairs are determined by and uniquely determine their first and second coordinates, that Cartesian products can be formed, and that every set of ordered pairs is a subset of some Cartesian product; which particular approach is used to achieve these ends is immaterial.\nIt is easy to locate the source of the mistrust and suspicion that many mathematicians feel toward the explicit definition of ordered pair given above. The trouble is not that there is anything wrong or anything missing; the relevant properties of the concept we defined are all correct (that is, in accord with the demands of intuition) and all the correct properties are present. The trouble is that the concept has some irrelevant properties that are accidental and distracting. The theorem that \\((a, b) = (x, y)\\) if and only if \\(a = x\\) and \\(b = y\\) is the sort of thing we expect to learn about ordered pairs. The fact that \\(\\{ a, b \\} \\in (a,b)\\), on the other hand, seems accidental; it is a freak property of the definition rather than an intrinsic property of the concept.\nThe charge of artificiality is true; but it is not too high a price to pay for conceptual economy. The concept of an ordered pair could have been introduced as an additional primitive, axiomatically endowed with just the right properties, no more and no less. In some theories this is done. The mathematician’s choice is between having to remember a few more axioms and having to forget a few accidental facts; the choice is pretty clearly a matter of taste. Similar choices occur frequently in mathematics; in this book, for instance, we shall encounter them again in connection with the definitions of numbers of various kinds.\n\nExercise 6.1 If \\(A\\), \\(B\\), \\(X\\), and \\(Y\\) are sets, then\n\\[\\begin{align*}\n(i)   & \\: (A \\cup B) \\times X = ( A \\times X ) \\cup ( B \\times X), \\\\\n(ii)  & \\: (A \\cap B ) \\times ( X \\cap Y) = (A \\times X) \\cap ( B \\times Y), \\\\\n(iii) & \\: (A - B ) \\times X = (A \\times X) - (B \\times X).\n\\end{align*}\\]\nIf either \\(A = \\emptyset\\) or \\(B = \\emptyset\\), then \\(A \\times B = \\emptyset\\), and conversely. If \\(A \\subset X\\) and \\(B \\subset Y\\), then \\(A \\times B \\subset X \\times Y\\), and (provided \\(A \\times B \\neq \\emptyset\\)) conversely.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Ordered Pairs</span>"
    ]
  },
  {
    "objectID": "chapter_7.html",
    "href": "chapter_7.html",
    "title": "7  Relations",
    "section": "",
    "text": "Using ordered pairs, we can formulate the mathematical theory of relations in set-theoretic language. By a relation we mean here something like marriage (between men and women) belonging (between elements and sets). More explicitly, what we shall call a relation is sometimes called a binary relation. An example of a ternary relation is parenthood for people (Adam and Eve are the parents of Cain). In this book we shall have no occasion to treat the theory of relations that are ternary, quaternary, or worse.\nLooking at any specific relation, such as marriage for instance, we might be tempted to consider certain ordered pairs \\((x,y)\\), namely just those for which \\(x\\) is man, \\(y\\) is a woman, and \\(x\\) is married to \\(y\\). We have not yet seen the definition of the general concept of a relation, but it seems plausible that, just as in this marriage example, every relation should uniquely determine the set of all those ordered pairs for which the first coordinate does stand in that relation to the second. If we know the relation, we know the set, and, better yet, if we know the set, we know the relation. If, for instance, we were presented with the set of ordered pairs of people that corresponds to marriage, then, even if we forgot the definition of marriage, we could always tell when a man \\(x\\) is married to a woman \\(y\\) and when not; we would just have to see whether the ordered pair \\((x,y)\\) does or does not belong to the set.\nWe may not know what a relation is, but we do know what a set is, and the preceding considerations establish a close connection between relations and sets. The precise set-theoretic treatment of relations takes advantage of that heuristic connection; the simplest to do is to define a relation to be the corresponding set. This is what we do; we hereby define a relation as a set of ordered pairs. Explicitly: a set \\(R\\) is a relation if each element of \\(R\\) is an ordered pair; this means, of course, that if \\(z \\in R\\), then there exist \\(x\\) and \\(y\\) so that \\(z = (x,y)\\). If \\(R\\) is relation, it is sometimes convenient to express the fact that \\((x, y) \\in R\\) by writing\n\\[\nxRy\n\\]\nand saying, as in everyday language, that \\(x\\) stands in the relation \\(R\\) to \\(y\\).\nThe least exciting relation is the empty one. (To prove that \\(\\emptyset\\) is a set of ordered pairs, look for an element of \\(\\emptyset\\) that is not an ordered pair.) Another dull example is the Cartesian product of any two sets \\(X\\) and \\(Y\\). Here is a slightly more interesting example: let \\(X\\) be any set, and let \\(R\\) be the set of all those pairs \\((x, y)\\) in \\(X \\times X\\) for which \\(x = y\\). The relation \\(R\\) is just the relation of equality between elements of \\(X\\); if \\(x\\) and \\(y\\) are in \\(X\\), then \\(xRy\\) means the same as \\(x = y\\). One more example will suffice for now: let \\(X\\) be any set, and let \\(R\\) be the set of all those pairs \\((x, A)\\) in \\(X \\times \\mathcal{P} (X)\\) for which \\(x \\in A\\). This relation \\(R\\) is just the relation of belonging between elements of \\(X\\) and subsets of \\(X\\); if \\(x \\in X\\) and \\(A \\in \\mathcal{P}(X)\\), then \\(x RA\\) means the same as \\(x \\in A\\).\nIn the preceding section we saw that associated with every set \\(R\\) of ordered pairs there are two sets called the projections of \\(R\\) onto the first and second coordinates. In the theory of relations these sets are known as the domain and the range of \\(R\\) (abbreviated \\(\\text{dom } R\\) and \\(\\text{ran } R\\)); we recall that they are defined by\n\\[\n\\text{dom } R = \\{ x: \\mathit{for\\ some\\ } y \\: (x R y) \\}\n\\]\nand\n\\[\n\\text{ran } R = \\{ y: \\mathit{\\ for\\ some\\ } x \\: (x R y) \\}.\n\\]\nIf \\(R\\) is the relation of marriage, so that \\(xRy\\) means that \\(x\\) is a man, \\(y\\) is a woman, and \\(x\\) and \\(y\\) are married to one another; then \\(\\text{dom } R\\) is the set of married men and \\(\\text{ran } R\\) is the set of married womem. Both the domain and the range of \\(\\emptyset\\) are equal to \\(\\emptyset\\). If \\(R = X \\times Y\\), then \\(\\text{dom } R = X\\) and \\(\\text{ran } R = Y\\). If \\(R\\) is equality in \\(X\\), then \\(\\text{dom } R = \\text{ran } R = X\\). If \\(R\\) is belonging, between \\(X\\) and \\(\\mathcal{P}(X)\\), then \\(\\text{dom } R = X\\) and \\(\\text{ran } R = \\mathcal{P}(X) - \\{ \\emptyset \\}\\).\nIf \\(R\\) is a relation included in a Cartesian product \\(X \\times Y\\) (so that \\(\\text{dom } R \\subset X\\) and \\(\\text{ran } R \\subset Y\\)), it is sometimes convenient to say that \\(R\\) is a relation from \\(X\\) to \\(Y\\); instead of a relation from \\(X\\) to \\(X\\) we may speak of a relation in \\(X\\). A relation \\(R\\) in \\(X\\) is reflexive if \\(xRx\\) for every \\(x\\) in \\(X\\); it is symmetric if \\(xRy\\) implies that \\(yRx\\); and it is transitive if \\(xRy\\) and \\(yRz\\) imply that \\(xRz\\). (Exercise: for each of these three possible properties, find a relation that does not have that property but does have the other two.) A relation in a set is an equivalence relation if it is reflexive, symmetric, and transitive. The smallest equivalence relation in a set \\(X\\) is the relation of equality in \\(X\\); the largest equivalence relation in \\(X\\) is \\(X \\times X\\).\nThere is an intimate connection between equivalence relations in a set \\(X\\) and certain collections (called partitions) of subsets of \\(X\\). A partition of \\(X\\) is a disjoint collection \\(\\mathcal{C}\\) of non-empty subsets of \\(X\\) whose union is \\(X\\). If \\(R\\) is an equivalence relation in \\(X\\), and if \\(x\\) is in \\(X\\), the equivalence class of \\(x\\) with respect to \\(R\\) is the set of all those elements \\(y\\) in \\(X\\) for which \\(xRy\\). (The weight of tradition makes the use of the word “class” at this point unavoidable.) Examples: if \\(R\\) is equality in \\(X\\), then each equivalence class is a singleton; if \\(R = X \\times X\\), then the set \\(X\\) itself is the only equivalence class. There is no standard notation for the equivalence class of \\(x\\) with respect to \\(R\\); we shall usually denote it by \\(x/R\\), and we shall write \\(X/R\\) for the set of all equivalence classes. (Pronounce \\(X/R\\) as “\\(X\\) modulo \\(R\\),” or, in abbreviated form, “\\(X \\text{ mod } R\\).” Exercise: show that \\(X/R\\) is indeed a set by exhibiting a condition that specifies exactly the subset \\(X/R\\) of the power set \\(\\mathcal{P}(X)\\).) Now forget \\(R\\) for a moment and begin anew with a partition \\(\\mathcal{C}\\) of \\(X\\). A relation, which we shall call \\(X/\\mathcal{C}\\), is defined in \\(X\\) by writing\n\\[\nx \\: X/ \\mathcal{C} \\: y\n\\]\njust in case \\(x\\) and \\(y\\) belong to the same set of the collection \\(\\mathcal{C}\\). We shall call \\(X/\\mathcal{C}\\) the relation induced by the partition \\(\\mathcal{C}\\).\nIn the preceding paragraph we saw how to associate a set of subsets of \\(X\\) with every equivalence relation in \\(X\\) and how to associate a relation in \\(X\\) with every partition of \\(X\\). The connection between equivalence relations and partitions can be described by saying that the passage from \\(\\mathcal{C}\\) to \\(X/\\mathcal{C}\\) is exactly the reverse of the passage from \\(R\\) to \\(X/R\\). More explicitly: if \\(R\\) is an equivalence relation in \\(X\\), then the set of equivalence classes is a partition of \\(X\\) that induces the relation \\(R\\), and if \\(\\mathcal{C}\\) is a partition of \\(X\\), then the induced relation is an equivalence relation whose set of equivalence classes is exactly \\(\\mathcal{C}\\).\nFor the proof, let us start with an equivalence relation \\(R\\). Since each \\(x\\) belongs to some equivalence class (for instance \\(x \\in x/R\\)), it is clear that the union of the equivalence classes is all \\(X\\). If \\(z \\in x/R \\cap y/ R\\), then \\(xRz\\) and \\(zRy\\), and therefore \\(xRy\\). This implies that if two equivalence classes have an element in common, then they are identical, or, in other words, that two distinct equivalence classes are always disjoint. The set of equivalence classes is therefore a partition. To say that two elements belong to the same set (equivalence class) of this partition means, by definition, that they stand in the relation \\(R\\) to one another. This proves the first half of our assertion.\nThe second half is easier. Start with a partition \\(\\mathcal{C}\\) and consider the induced relation. Since every element of \\(X\\) belongs to some set of \\(\\mathcal{C}\\), reflexivity just says that \\(x\\) and \\(x\\) are in the same set of \\(\\mathcal{C}\\). Symmetry says that if \\(x\\) and \\(y\\) are in the same set of \\(\\mathcal{C}\\), then \\(y\\) and \\(x\\) are in the same set of \\(\\mathcal{C}\\), and this is obviously true. Transitivity says that if \\(x\\) and \\(y\\) are in the same set of \\(\\mathcal{C}\\) and if \\(y\\) and \\(z\\) are in the same set of \\(\\mathcal{C}\\), then \\(x\\) and \\(z\\) are in the same set of \\(\\mathcal{C}\\), and this too is obvious. The equivalence class of each \\(x\\) in \\(X\\) is just the set of \\(\\mathcal{C}\\) to which \\(x\\) belongs. This completes the proof of everything that was promised.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Relations</span>"
    ]
  },
  {
    "objectID": "chapter_8.html",
    "href": "chapter_8.html",
    "title": "8  Functions",
    "section": "",
    "text": "If \\(X\\) and \\(Y\\) are sets, a function from (or on) \\(X\\) to (or into) \\(Y\\) is a relation \\(f\\) such that \\(\\text{dom } f = X\\) and such that for each \\(x\\) in \\(X\\) there is a unique element \\(y\\) in \\(Y\\) with \\((x, y) \\in f\\). The uniqueness condition can be formulated explicitly as follows: if \\((x, y) \\in f\\) and \\((x, z) \\in f\\), then \\(y = z\\). For each \\(x\\) in \\(X\\), the unique \\(y\\) in \\(Y\\) such that \\((x, y) \\in f\\) is denoted by \\(f(x)\\). For functions this notation and its minor variants supersede the others used for more general relations; from now on, if \\(f\\) is a function, we shall write \\(f(x) = y\\) instead of \\((x, y) \\in f\\) or \\(xfy\\). The element \\(y\\) is called the value that the function \\(f\\) assumes (or takes on) at the argument \\(x\\); equivalently we may say that \\(f\\) sends or maps or transforms \\(x\\) into \\(y\\). The words map or mapping, transformation, correspondence, and operator are among some of the many that are sometimes used as synonyms for function. The symbol\n\\[\nf: X \\rightarrow Y\n\\]\nis sometimes used as an abbreviation for “\\(f\\) is a function from \\(X\\) to \\(Y\\).” The set of all functions \\(X\\) to \\(Y\\) is a subset of the power set \\(\\mathcal{P}(X \\times Y)\\); it will be denoted by \\(Y^X\\).\nThe connotations of activity suggested by the synonyms listed above make some scholars dissatisfied with the definition according to which function does not do anything but merely is. This dissatisfaction is reflected in a different use of the vocabulary: function is reserved for the undefined object that is somehow active, and the set of ordered pairs that we have called the function is then called the graph of the function. It is easy to find examples of functions in the precise set-theoretic sense of the word in both mathematics and everyday life; all we have to look for is information, not necessarily numerical, in tabulated form. One example is a city directory; the arguments of the function are, in this case, the inhabitants of the city, and the values are their addresses.\nFor relations in general, and hence for functions in particular, we have defined the concepts of domain and range. The domain of a function \\(f\\) from \\(X\\) into \\(Y\\) is, by definition, equal to \\(X\\), but its range need not be equal to \\(Y\\); the range consists of those elements \\(y\\) of \\(Y\\) for which there exists an \\(x\\) in \\(X\\) such that \\(f(x) = y\\). If the range of \\(f\\) is equal to \\(Y\\), we say that \\(f\\) maps \\(X\\) onto \\(Y\\). If \\(A\\) is a subset of \\(X\\), we may want to consider the set of all those elements \\(y\\) of \\(Y\\) for which there exists an \\(x\\) in the subset \\(A\\) such that \\(f(x) = y\\). This subset of \\(Y\\) is called the image of \\(A\\) under \\(f\\) and is frequently denoted by \\(f(A)\\). The notation is bad but not catastrophic. What is bad about it is that if \\(A\\) happens to be both an element of \\(X\\) and a subset of \\(X\\) (an unlikely situation, but far from an impossible one), then the symbol \\(f(A)\\) is ambiguous. Does it mean the value of \\(f\\) at \\(A\\) or does it mean the set of values of \\(f\\) at the elements of \\(A\\)? Following normal mathematical custom, we shall use the bad notation, relying on context, and, on the rare occasions when it is necessary, adding verbal stipulations, to avoid confusion. Note that the image of \\(X\\) itself is the range of \\(f\\); the “onto” character of \\(f\\) can be expressed by writing \\(f(X) = Y\\).\nIf \\(X\\) is a subset of a set \\(Y\\), the function \\(f\\) defined by \\(f(x) = x\\) for each \\(x\\) in \\(X\\) is called the inclusion map (or embedding, or the injection) of \\(X\\) into \\(Y\\). The phrase “the function \\(f\\) defined by …” is a very common one in such contexts. It is intended to imply, of course, that there does indeed exist a unique function satisfying the stated condition. In the special case at hand this is obvious enough; we are being invited to consider the set of all those ordered pairs \\((x, y)\\) in \\(X \\times Y\\) for which \\(x = y\\). Similar considerations apply in every case, and, following normal mathematical practice, we shall usually describe a function by describing its value \\(y\\) at each argument \\(x\\). Such a description is sometimes longer and more cumbersome than a direct description of the set (of ordered pairs) involved, but, nevertheless, most mathematicians regard the argument-value description as more perspicuous than any other.\nThe inclusion map of \\(X\\) into \\(X\\) is called the identity map on \\(X\\). (In the language of relations, the identity map on \\(X\\) is the same as the relation of equality in \\(X\\).) If, as before, \\(X \\subset Y\\), then there is a connection between the inclusion map of \\(X\\) into \\(Y\\) and the identity map on \\(Y\\); that connection is a special case of a general procedure for making small functions out of large ones. If \\(f\\) is a function from \\(Y\\) to \\(Z\\), say, and if \\(X\\) is a subset of \\(Y\\), then there is a natural way of constructing a function \\(g\\) from \\(X\\) to \\(Z\\); define \\(g(x)\\) to be equal to \\(f(x)\\) for each \\(x\\) in \\(X\\). The function \\(g\\) is called the restriction of \\(f\\) to \\(X\\), and \\(f\\) is called an extension of \\(g\\) to \\(Y\\); it is customary to write \\(g = f \\mid X\\). The definition of restriction can be expressed by writing \\((f \\mid X)(x) = f(x)\\) for each \\(x\\) in \\(X\\); observe also that \\(\\text{ran }(f \\mid X) = f(X)\\). The inclusion map of a subset of \\(Y\\) is the restriction to that subset of the identity map on \\(Y\\).\nHere is a simple but useful example of a function. Consider any two sets \\(X\\) and \\(Y\\), and define a function \\(f\\) from \\(X \\times Y\\) onto \\(X\\) by writing \\(f(x,y) = x\\). (The purist will have noted that we should have written \\(f((x, y))\\) instead of \\(f(x, y)\\), but nobody ever does.) The function \\(f\\) is called the projection from \\(X \\times Y\\) onto \\(X\\); if, similarly, \\(g(x, y) = y\\), then \\(g\\) is the projection from \\(X \\times Y\\) onto \\(Y\\). The terminology here is at variance with an earlier one, but not badly. If \\(R = X \\times Y\\), then what was earlier called the projection of \\(R\\) onto the first coordinate is, in the present language, the range of the projection \\(f\\).\nA more complicated and correspondingly more valuable example of a function can be obtained as follows. Suppose \\(R\\) is an equivalence relation in \\(X\\), and let \\(f\\) be the function from \\(X\\) onto \\(X/R\\) defined by \\(f(x) = x/R\\). The function \\(f\\) is sometimes called the canonical map from \\(X\\) to \\(X/R\\).\nIf \\(f\\) is an arbitrary function, from \\(X\\) onto \\(Y\\), then there is a natural way of defining an equivalence relation \\(R\\) in \\(X\\); write \\(aRb\\) (where \\(a\\) and \\(b\\) are in \\(X\\)) in case \\(f(a) = f(b)\\). For each element \\(y\\) of \\(Y\\), let \\(g(y)\\) be the set of all those elements \\(x\\) in \\(X\\) for which \\(f(x) = y\\). The definition of \\(R\\) implies that \\(g(y)\\) is, for each \\(y\\), an equivalence class of the relation \\(R\\); in other words, \\(g\\) is a function from \\(Y\\) onto the set \\(X/R\\) of all equivalence classes of \\(R\\). The function \\(g\\) has the following special property: if \\(u\\) and \\(v\\) are distinct elements of \\(Y\\), then \\(g(u)\\) and \\(g(v)\\) are distinct elements of \\(X/R\\). A function that always maps distinct elements onto distinct elements is called one-to-one (usually a one-to-one correspondence). Among the examples above the inclusion maps are one-to-one, but, except in some trivial special cases, the projections are not. (Exercise: what special cases?)\nTo introduce the next aspect of the elementary theory of functions we must digress for a moment and anticipate a tiny fragment of our ultimate definition of natural numbers. We shall not find it necessary to define all the natural numbers now; all we need is the first three of them. Since this is not the appropriate occasion for lengthy heuristic preliminaries, we shall proceed directly to the definition, even at the risk of temporarily shocking or worrying some readers. Here it is: we define \\(0\\), \\(1\\), and \\(2\\) by writing\n\\[\n0 = \\emptyset , \\: 1=\\{ \\emptyset \\} , \\: \\text{and} \\: 2 = \\{ \\emptyset , \\{ \\emptyset \\} \\} .\n\\]\nIn other words, \\(0\\) is empty, \\(1\\) is the singleton \\(\\{ 0 \\}\\), and \\(2\\) is the pair \\(\\{ 0, 1 \\}\\). Observe that there is some method in this apparent madness; the number of elements in the sets \\(0\\), \\(1\\), or \\(2\\) (in the ordinary everyday sense of the word) is, respectively, zero, one, or two.\nIf \\(A\\) is a subset of a set \\(X\\), the characteristic function of \\(A\\) is the function \\(\\chi\\) from \\(X\\) to \\(2\\) such that \\(\\chi(x) = 1\\) or \\(0\\) according as \\(x \\in A\\) or \\(x \\in X - A\\). The dependence of the characteristic function of \\(A\\) on the set \\(A\\) may be indicated by writing \\(\\chi_A\\) instead of \\(\\chi\\). The function that assigns to each subset \\(A\\) of \\(X\\) (that is, to each element of \\(\\mathcal{P}(X)\\)) the characteristic function of \\(A\\) (that is an element of \\(2^X\\)) is a one-to-one correspondence between \\(\\mathcal{P}(X)\\) and \\(2^X\\). (Parenthetically: instead of the phrase “the function that assigns to each \\(A\\) in \\(\\mathcal{P}(X)\\) the element \\(\\chi_A\\) in \\(2^X\\)” it is customary to use the abbreviation “the function \\(A \\rightarrow \\chi_A\\).” In this language, the projection from \\(X \\times Y\\) onto \\(X\\), for instance, may be called the function \\((x, y) \\rightarrow x\\), and the canonical map from a set \\(X\\) with a relation \\(R\\) onto \\(X/R\\) may be called the function \\(x \\rightarrow x/R\\).)\n\nExercise 8.1 \\((i)\\) \\(Y^{\\emptyset}\\) has exactly one element, namely \\(\\emptyset\\), wheter \\(Y\\) is empty or not, and \\((ii)\\) if \\(X\\) is not empty, then \\(\\emptyset^X\\) is empty.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Functions</span>"
    ]
  },
  {
    "objectID": "chapter_9.html",
    "href": "chapter_9.html",
    "title": "9  Families",
    "section": "",
    "text": "There are occasions when the range of a function is deemed to be more important than the function itself. When that is the case, both the terminology and the notation undergo radical alterations. Suppose, for instance, that \\(x\\) is a function from a set \\(I\\) to a set \\(X\\). (The very choice of letters indicates that something strange is afoot.) An element of the domain \\(I\\) is called an index, \\(I\\) is called the index set, the range of the function is called an indexed set, the function itself is called a family, and the value of the function \\(x\\) at an index \\(i\\), called a term of the family, is denoted by \\(x_i\\). (This terminology is not absolutely established, but it is one of the standard choices among related slight variants; in the sequel it and it alone will be used.) An unacceptable but generally accepted way of communicating the notation and indicating the emphasis is to speak of a family \\(\\{ x_{i} \\}\\) in \\(X\\), or of a family \\(\\{ x_{i} \\}\\) of whatever the elements of \\(X\\) may be; when necessary, the index set \\(I\\) is indicated by some such parenthetical expression as \\((i \\in I)\\). Thus, for instance, the phrase “a family \\(\\{ A_{i} \\}\\) of subsets of \\(X\\)” is usually understood to refer to a function \\(A\\), from some set \\(I\\) of indices, into \\(\\mathcal{P}(X)\\).\nIf \\(\\{ A_{i} \\}\\) is a family of subsets of \\(X\\), the union of the range of the family is called the union of the family \\(\\{ A_{i} \\}\\), or the union of the sets \\(A_{i}\\); the standard notation for it is\n\\[\n\\bigcup_{i \\in I}A_{i} \\: \\text{ or } \\: \\bigcup_{i}A_{i},\n\\]\naccording as it is or is not important to emphasize the index set \\(I\\). It follows immediately from the definition of unions that \\(x \\in \\bigcup_{i} A_{i}\\) if and only if \\(x\\) belongs to \\(A_{i}\\) for at least one \\(i\\). If \\(I = 2\\), so that the range of the family \\(\\{ A_{i} \\}\\) is the unordered pair \\(\\{ A_{0}, A_{i} \\}\\), then \\(\\bigcup_{i}A_{i} =  A_{0} \\cup A_{1}\\). Observe that there is no loss of generality in considering families of sets instead of arbitrary collections of sets; every collection of sets is the range of some family. If, indeed, \\(\\mathcal{C}\\) is a collection of sets, let \\(\\mathcal{C}\\) itself play the role of the index set, and consider the identity mapping on \\(\\mathcal{C}\\) in the role the family.\nThe algebraic laws satisfied by the operation of union for pairs can be generalized to arbitrary unions. Suppose, for instance, that \\(\\{ I_{j} \\}\\) is a family of sets with domain \\(J\\), say; write \\(K = \\bigcup_{j}I_{j}\\), and let \\(\\{ A_{k} \\}\\) be a family of sets with domain \\(K\\). It is then not difficult to prove that\n\\[\n\\bigcup_{k \\in K}A_{k} = \\bigcup_{j \\in J}\\left( \\bigcup_{i \\in I_{j}}A_{i} \\right);\n\\]\nthis is the generalized version of the associative law for unions. Exercise: formulate and prove a generalized version of the commutative law.\nAn empty union makes sense (and is empty), but an empty intersection does not make sense. Except for this triviality, the terminology and notation for intersections parallels that for unions in every respect. Thus, for instance, if \\(\\{ A_{i} \\}\\) is a non-empty family of sets, the intersection of the range of the family is called the intersection of the family \\(\\{ A_{i} \\}\\), or the intersection of the sets \\(A_{i}\\); the standard notation for it is\n\\[\n\\bigcap_{i \\in I}A_{i} \\: \\text{ or } \\: \\bigcap_{i}A_{i},\n\\]\naccording as it is or is not important to emphasize the index set \\(I\\). (By a “non-empty family” we mean a family whose domain \\(I\\) is not empty.) It follows immediately from the definition of intersections that if \\(I \\neq \\emptyset\\), then a necessary and sufficient condition that \\(x\\) belong \\(\\bigcap_{i}A_{i}\\) is that \\(x\\) belong to \\(A_{i}\\) for all \\(i\\).\nThe generalized commutative and associative laws for intersections can be formulated and proved the same way as for unions, or, alternatively, De Morgan’s laws can be used to derive them from the facts for unions. This is almost obvious, and, therefore, it is not of much interest. The interesting algebraic identities are the ones that involve both unions and intersections. Thus, for instance, if \\(\\{ A_{i} \\}\\) is a family of subsets of \\(X\\) and \\(B \\subset X\\), then\n\\[\nB \\cap \\bigcup_{i}A_{i} = \\bigcup_{i}(B \\cap A_{i})\n\\]\nand\n\\[\nB \\cup \\bigcap_{i}A_{i} = \\bigcap_{i}(B \\cup A_{i});\n\\]\nthese equations are a mild generalization of the distributive laws.\n\nExercise 9.1 If both \\(\\{ A_{i} \\}\\) and \\(\\{ B_{i} \\}\\) are families of sets, then\n\\[\n\\left( \\bigcup_{i}A_{i} \\right) \\cap \\left( \\bigcup_{j}B_{j} \\right) = \\bigcup_{i,j}(A_{i} \\cap B_{j})\n\\]\nand\n\\[\n\\left( \\bigcap_{i}A_{i} \\right) \\cup \\left( \\bigcap_{j}B_{j} \\right) = \\bigcap_{i,j}(A_{i} \\cup B_{j}).\n\\]\nExplanation of notation: a symbol such as \\(\\bigcup_{i,j}\\) is an abbreviation for \\(\\bigcup_{(i,j) \\in I \\times J}\\).\n\nThe notation of families is the one normally used in generalizing the concept of Cartesian product. The Cartesian product of two sets \\(X\\) and \\(Y\\) was defined as the set of all ordered pairs \\((x, y)\\) with \\(x\\) in \\(X\\) and \\(y\\) in \\(Y\\). There is a natural one-to-one correspondence between this set and a certain set of families. Consider, indeed, any particular unordered pair \\(\\{ a,b \\}\\), with \\(a \\neq b\\), and consider the set \\(Z\\) of all families \\(z\\), indexed by \\(\\{ a,b \\}\\), such that \\(z_{a} \\in X\\) and \\(z_{b} \\in Y\\). If the function \\(f\\) from \\(Z\\) to \\(X \\times Y\\) is defined by \\(f(z) = (z_{a}, z_{b})\\), then \\(f\\) is the promised one-to-one correspondence. The difference between \\(Z\\) and \\(X \\times Y\\) is merely matter of notation. The generalization of Cartesian products generalizes \\(Z\\) rather than \\(X \\times Y\\) itself. (As a consequence there is a little terminological friction in the passage from the special case to the general. There is no help for it; that is how mathematical language is in fact used nowadays.) The generalization is now straightforward. If \\(\\{ X_{i} \\}\\) is a family of sets \\((i \\in I)\\), the Cartesian product of the family is, by definition, the set of all families \\(\\{ x_{i} \\}\\) with \\(x_{i} \\in X_{i}\\) for each \\(i\\) in \\(I\\). There are several symbols for the Cartesian product in more or less current usage; in this book we shall denote it by\n\\[\n\\bigtimes_{i \\in I}X_{i} \\: \\text{ or } \\: \\bigtimes_{i}X_{i}.\n\\]\nIt is clear that if every \\(X_{i}\\) is equal to one and the same set \\(X\\), then \\(\\bigtimes_{i}X_{i} = X^{I}\\). If \\(I\\) is a pair \\(\\{ a,b \\}\\), with \\(a \\neq b\\), then it is customary to identify \\(\\bigtimes_{i \\in I}X_{i}\\) with the Cartesian product \\(X_{a} \\times X_{b}\\) as defined earlier, and if \\(I\\) is a singleton \\(\\{ a \\}\\), then, similarly, we identify \\(\\bigtimes_{i \\in I}X_{i}\\) with \\(X_{a}\\) itself. Ordered triples, ordered quadruples, etc., may be defined as families whose index sets are unordered triples, quadruples, etc.\nSuppose that \\(\\{ X_{i} \\}\\) is a family of sets \\((i \\in I)\\) and let \\(X\\) be its Cartesian product. If \\(J\\) is a subset of \\(I\\), then to each element of \\(X\\) there corresponds in a natural way an element of the partial Cartesian product \\(\\bigtimes_{i \\in J}X_{i}\\). To define the correspondence, recall that each element \\(x\\) of \\(X\\) is itself a family \\(\\{ x_{i} \\}\\), that is, in the last analysis, a function on \\(I\\); the corresponding element, say \\(y\\), of \\(\\bigtimes_{i \\in J}X_{i}\\) is obtained by simply restricting that function to \\(J\\). Explicitly, we write \\(y_{i} = x_{i}\\) whenever \\(i \\in J\\). The correspondence \\(x \\rightarrow y\\) is called the projection from \\(X\\) onto \\(\\bigtimes_{i \\in J}X_{i}\\); we shall temporarily denote it by \\(f_{J}\\). If, in particular, \\(J\\) is a singleton, say \\(J = \\{ j \\}\\), then we shall write \\(f_{j}\\) (instead of \\(f_{ \\{j \\} }\\)) for \\(f_{J}\\). The word “projection” has a multiple use; if \\(x \\in X\\), the value of \\(f_{j}\\) at \\(x\\), that is \\(x_{j}\\), is also called the projection of \\(x\\) onto \\(X_{j}\\), or, alternatively, the \\(j\\)-coordinate of \\(x\\). A function on a Cartesian product such as \\(X\\) is called a function of several variables, and, in particular, a function on a Cartesian product \\(X_{a} \\times X_{b}\\) is called a function of two variables.\n\nExercise 9.2 Prove that \\((\\bigcup_{i}A_{i}) \\times (\\bigcup_{j}B_{j}) = \\bigcup_{i,j}(A_{i} \\times B_{j})\\), and that the same equation holds for intersections (provided that the domains of the families involved are not empty). Prove also (with appropriate provisos about empty families) that \\(\\bigcap_{i}X_{i} \\subset X_{j} \\subset \\bigcup_{i}X_{i}\\) for each index \\(j\\) and that intersection and union can in fact be characterized as the extreme solutions of these inclusions. This means that if \\(X_{j} \\subset Y\\) for each index \\(j\\), then \\(\\bigcup_{i}X_{i} \\subset Y\\), and that \\(\\bigcup_{i}X_{i}\\) is the only set satisfying this minimality condition; the formulation for intersections is similar.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Families</span>"
    ]
  },
  {
    "objectID": "chapter_10.html",
    "href": "chapter_10.html",
    "title": "10  Inverses and Composites",
    "section": "",
    "text": "Associated with every function \\(f\\), from \\(X\\) to \\(Y\\), say, there is a function from \\(\\mathcal{P}(X)\\) to \\(\\mathcal{P}(Y)\\), namely the function (frequently called \\(f\\) also) that assigns to each subset \\(A\\) of \\(X\\) the image subset \\(f(A)\\) of \\(Y\\). The algebraic behavior of the mapping \\(A \\rightarrow f(A)\\) leaves something to be desired. It is true that if \\(\\{ A_{i} \\}\\) is a family of subsets of \\(X\\), then \\(f( \\bigcup_{i} A_{i}) = \\bigcup_{i} f( A_{i})\\) (proof?), but the corresponding equation for intersections is false in general (example?), and the connection between images and complements is equally unsatisfactory.\nA correspondence between the elements of \\(X\\) and the elements of \\(Y\\) does always induce a well-behaved correspondence between the subsets of \\(X\\) and the subsets of \\(Y\\), not forward, by the formation of images, but backward, by the formation of inverse images. Given a function \\(f\\) from \\(X\\) to \\(Y\\), let \\(f^{-1}\\), the inverse of \\(f\\), be the function from \\(\\mathcal{P}(Y)\\) to \\(\\mathcal{P}(X)\\) such that if \\(B \\subset Y\\), then\n\\[\nf^{-1}(B) = \\{ x \\in X : f(x) \\in B \\}.\n\\]\nIn words: \\(f^{-1}(B)\\) consists of exactly those elements of \\(X\\) that \\(f\\) maps into \\(B\\); the set \\(f^{-1}(B)\\) is called the inverse image of \\(B\\) under \\(f\\). A necessary and sufficient condition that \\(f\\) map \\(X\\) onto \\(Y\\) is that the inverse image under \\(f\\) of each non-empty subset of \\(Y\\) be a non-empty subset of \\(X\\). (Proof?) A necessary and sufficient condition that \\(f\\) be one-to-one is that the inverse image under \\(f\\) of each singleton in the range of \\(f\\) be a singleton in \\(X\\).\nIf the last condition is satisfied, then the symbol \\(f^{-1}\\) is frequently assigned a second interpretation, namely as the function whose domain is the range of \\(f\\), and whose value for each \\(y\\) in the range of \\(f\\) is the unique \\(x\\) in \\(X\\) for which \\(f(x) = y\\). In other words, for one-to-one functions \\(f\\) we may write \\(f^{-1}(y) = x\\) if and only if \\(f(x) = y\\). This use of the notation is mildly inconsistent with our first interpretation of \\(f^{-1}\\), but the double meaning is not likely to lead to any confusion.\nThe connection between images and inverse images is worth a moment’s consideration.\nIf \\(B \\subset Y\\), then\n\\[\nf(f^{-1}(B)) \\subset B.\n\\]\n\nProof. If \\(y \\in f(f^{-1}(B))\\), then \\(y = f(x)\\) for some \\(x\\) in \\(f^{-1}(B)\\); this means that \\(y = f(x)\\) and \\(f(x) \\in B\\), and therefore \\(y \\in B\\).\n\nIf \\(f\\) maps \\(X\\) onto \\(Y\\), then\n\\[\nf(f^{-1}(B)) = B.\n\\]\n\nProof. If \\(y \\in B\\), then \\(y = f(x)\\) for some \\(x\\) in \\(X\\), and therefore for some \\(x\\) in \\(f^{-1}(B)\\); this means that \\(y \\in f(f^{-1}(B))\\).\n\nIf \\(A \\subset X\\), then\n\\[\nA \\subset f^{-1}(f(A))\n\\]\n\nProof. If \\(x \\in A\\), then \\(f(x) \\in f(A)\\); this means that \\(x \\in f^{-1}(f(A))\\).\n\nIf \\(f\\) is one-to-one, then\n\\[\nA = f^{-1}(f(A))\n\\]\n\nProof. If \\(x \\in f^{-1}(f(A))\\), then \\(f(x) \\in f(A)\\), and therefore \\(f(x) = f(u)\\) for some \\(u\\) in \\(A\\); this implies that \\(x = u\\) and hence that \\(x \\in A\\).\n\nThe algebraic behavior of \\(f^{-1}\\) is unexceptionable. If \\(\\{ B_{i} \\}\\) is a family of subsets of \\(Y\\), then\n\\[\nf^{-1}\\left( \\bigcup_{i} B_{i} \\right) = \\bigcup_{i}f^{-1}(B_{i})\n\\]\nand\n\\[\nf^{-1}\\left( \\bigcap_{i} B_{i} \\right) = \\bigcap_{i}f^{-1}(B_{i})\n\\]\nThe proofs are straightforward. If, for instance, \\(x \\in f^{-1}( \\bigcap_{i} B_{i})\\), then \\(f(x) \\in B_{i}\\) for all \\(i\\), so that \\(x \\in f^{-1}(B_{i})\\) for all \\(i\\), and therefore \\(x \\in \\bigcap_{i}f^{-1}(B_{i})\\); all the steps in this argument are reversible. The formation of inverse images commutes with complementation also; i.e.,\n\\[\nf^{-1}(Y - B) = X - f^{-1}(B)\n\\]\nfor each subset \\(B\\) of \\(Y\\). Indeed: if \\(x \\in f^{-1}(Y - B)\\), then \\(f(x) \\in Y - B\\), so that \\(x \\notin f^{-1}(B)\\), and therefore \\(x \\in X - f^{-1}(B)\\); the steps are reversible. (Observe that the last equation is indeed a kind of commutative law: it says that complementation followed by inversion is the same as inversion followed by complementation.)\nThe discussion of inverses shows that what a function does can in a certain sense be undone; the next thing we shall see is that what two functions do can sometimes be done in one step. If, to be explicit, \\(f\\) is a function from \\(X\\) to \\(Y\\) and \\(g\\) is a function from \\(Y\\) to \\(Z\\), then every element in the range of \\(f\\) belongs to the domain of \\(g\\), and, consequently, \\(g(f(x))\\) makes sense for each \\(x\\) in \\(X\\). The function \\(h\\) from \\(X\\) to \\(Z\\), defined by \\(h(x) = g(f(x))\\) is called the composite of the functions \\(f\\) and \\(g\\); it is denoted by \\(g \\circ f\\) or, more simply, by \\(gf\\). (Since we shall not have occasion to consider any other kind of multiplication for functions, in this book we shall use the latter, simpler notation only.)\nObserve that the order of events is important in the theory of functional composition. In order that \\(gf\\) be defined, the range of \\(f\\) must be included in the domain of \\(g\\), and this can happen without it necessarily happening in the other direction at the same time. Even if both \\(fg\\) and \\(gf\\) are defined, which happens if, for instance, \\(f\\) maps \\(X\\) into \\(Y\\) and \\(g\\) maps \\(Y\\) into \\(X\\), the functions \\(fg\\) and \\(gf\\) need not be the same; in other words, functional composition is not necessarily commutative.\nFunctional composition may not be commutative, but it is always associative. If \\(f\\) maps \\(X\\) into \\(Y\\), if \\(g\\) maps \\(Y\\) into \\(Z\\), and if \\(h\\) maps \\(Z\\) into \\(U\\), then we can form the composite of \\(h\\) with \\(gf\\) and the composite of \\(hg\\) with \\(f\\); it is a simple exercise to show that the result is the same in either case.\nThe connection between inversion and composition is important; something like it crops up all over mathematics. If \\(f\\) maps \\(X\\) into \\(Y\\) and \\(g\\) maps \\(Y\\) into \\(Z\\), then \\(f^{-1}\\) maps \\(\\mathcal{P}(Y)\\) into \\(\\mathcal{P}(X)\\) and \\(g^{-1}\\) maps \\(\\mathcal{P}(Z)\\) into \\(\\mathcal{P}(Y)\\). In this situation, the composites that are formable are \\(gf\\) and \\(f^{-1}g^{-1}\\); the assertion is that the latter is the inverse of the former. Proof: if \\(x \\in (gf)^{-1}(C)\\), where \\(x \\in X\\) and \\(C \\subset Z\\), then \\(g(f(x)) \\in C\\), so that \\(f(x) \\in g^{-1}(C)\\), and therefore \\(x \\in f^{-1}(g^{-1}(C))\\); the steps of the argument are reversible.\nInversion and composition for functions are special cases of similar operations for relations. Thus, in particular, associated with every relation \\(R\\) from \\(X\\) to \\(Y\\) there is the inverse (or converse) relation \\(R^{-1}\\) from \\(Y\\) to \\(X\\); by definition \\(y R^{-1} x\\) means that \\(xRy\\). Example: if \\(R\\) is the relation of belonging, from \\(X\\) to \\(\\mathcal{P}(X)\\), then \\(R^{-1}\\) is the relation of containing, from \\(\\mathcal{P}(X)\\) to \\(X\\). It is an immediate consequence of the definitions involved that \\(\\text{dom }R^{-1} = \\text{ran } R\\) and \\(\\text{ran }R^{-1} = \\text{dom } R\\). If the relation \\(R\\) is a function, then the equivalent assertions \\(xRy\\) and \\(yR^{-1}x\\) can be written in the equivalent forms \\(R(x) = y\\) and \\(x \\in R^{-1}( \\{ y \\} )\\).\nBecause of difficulties with commutativity, the generalization of functional composition has to be handled with care. The composite of the relations \\(R\\) and \\(S\\) is defined in case \\(R\\) is a relation from \\(X\\) to \\(Y\\) and \\(S\\) is a relation from \\(Y\\) to \\(Z\\). The composite relation \\(T\\), from \\(X\\) to \\(Z\\), is denoted by \\(S \\circ R\\), or, simply, by \\(SR\\); it is defined so that \\(xTz\\) if and only if there exists an element \\(y\\) in \\(Y\\) such that \\(xRy\\) and \\(ySz\\). For an instructive example, let \\(R\\) mean “son” and let \\(S\\) mean “brother” in the set of human males. In other words, \\(xRy\\) means that \\(x\\) is son of \\(y\\), and \\(ySz\\) means that \\(y\\) is a brother of \\(z\\). In this case the composite relation \\(SR\\) means “nephew.” (Query: what do \\(R^{-1}\\), \\(S^{-1}\\), \\(RS\\), and \\(R^{-1}S^{-1}\\) mean?) If both \\(R\\) and \\(S\\) are functions, then \\(xRy\\) and \\(ySz\\) can be rewritten as \\(R(x) = y\\) and \\(S(y) = z\\) respectively. It follows that \\(S(R(x)) = z\\) if and only if \\(xTz\\), so that functional composition is indeed a special case of what is sometimes called the relative product.\nThe algebraic properties of inversion and composition are the same for relations as for functions. Thus, in particular, composition is commutative by accident only, but it is always associative, and it is always connected with inversion via the equation \\((SR)^{-1} = R^{-1}S^{-1}\\). (Proofs?)\nThe algebra of relations provides some amusing formulas. Suppose that, temporarily, we consider relations in one set \\(X\\) only, and, in particular, let the relation of equality in \\(X\\) (which is the same as the identity mapping on \\(X\\)). The relation \\(I\\) acts as a multiplicative unit; this means that \\(IR = RI = R\\) for every relation \\(R\\) in \\(X\\). Query: is there a connection among \\(I\\), \\(RR^{-1}\\), and \\(R^{-1}R\\)? The three defining properties of an equivalence relation can be formulated in algebraic terms as follows: reflexivity means \\(I \\subset R\\), symmetry means \\(R \\subset R^{-1}\\), and transitivity means \\(RR \\subset R\\).\n\nExercise 10.1 (Assume in each case that \\(f\\) is a function from \\(X\\) to \\(Y\\)) \\((i)\\) If \\(g\\) is a function from \\(Y\\) to \\(X\\) such that \\(gf\\) is the identity on \\(X\\), then \\(f\\) is one-to-one and \\(g\\) maps \\(Y\\) onto \\(X\\). \\((ii)\\) A necessary and sufficient condition that \\(f(A \\cap B) = f(A) \\cap f(B)\\) for all subsets \\(A\\) and \\(B\\) of \\(X\\) is that \\(f\\) be one-to-one. \\((iii)\\) necessary and sufficient condition that \\(f(X - A) \\subset Y - f(A)\\) for all subsets \\(A\\) of \\(X\\) is that \\(f\\) be one-to-one. \\((iv)\\) A necessary and sufficient condition that \\(Y - f(A) \\subset f(X - A)\\) for all subsets \\(A\\) of \\(X\\) is that \\(f\\) map \\(X\\) onto \\(Y\\).",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Inverses and Composites</span>"
    ]
  }
]